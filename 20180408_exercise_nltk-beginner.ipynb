{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi!', 'My name is Jane.', \"What's your name?\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para = \"Hi! My name is Jane. What's your name?\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #选择这个方法\n",
    "word_tokenize('Hello world.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize('hello world?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"Hello, my friend.\"\n",
    "pattern = r\"\\w+|[^\\w\\s]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'my', 'friend', '.']\n"
     ]
    }
   ],
   "source": [
    "print (nltk.tokenize.regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize sentences using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can', 't', 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer = RegexpTokenizer(\"[\\w]+\") \n",
    "#这里没有了\"’\"，所以can't分开了\n",
    "tokenizer.tokenize(\"Can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'w', 'o', 'n', \"'\", 't', 'b', 'e', 't', 'h', 'e', 'r', 'e']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"I won't be there!\",\"[\\w']\") #少了+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"won't\", 'be', 'there']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个方法比较简单\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"I won't be there!\",\"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#from nltk.corpus import webtext\n",
    "text =\"\"\"White guy: So, do you have any plans for this evening?\n",
    "Girl: But you already have a Big Mac...\n",
    "Hobo: Oh, this is all!\n",
    "\"\"\"\n",
    "sent_tokenizer=PunktSentenceTokenizer(text)\n",
    "sent1=sent_tokenizer.tokenize(text)\n",
    "\n",
    "sent2=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all!\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Girl: But you already have a Big Mac...\\nHobo: Oh, this is all!\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'White guy: So, do you have any plans for this evening?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-8013b7cc31b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msent1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sent1[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering stopwords in a tokenized sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"Won't\", 'Be', 'There']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops=set(stopwords.words('english'))\n",
    "words = ['I',\"Won't\",'Be','There']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops=set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', '$3']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops=set(stopwords.words('english'))\n",
    "words = ['I',\"have\",'$3']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up synsets 同义词集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookbook.n.01'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('cookbook')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a book of recipes and cooking directions'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car.n.01'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('motorcar')[0]\n",
    "syn.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordnet 层级结构\n",
    "from nltk.corpus import wordnet as wn\n",
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar=motorcar.hyponyms()\n",
    "types_of_motorcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ambulance.n.01'),\n",
       " Synset('beach_wagon.n.01'),\n",
       " Synset('bus.n.04'),\n",
       " Synset('cab.n.03'),\n",
       " Synset('compact.n.03'),\n",
       " Synset('convertible.n.01'),\n",
       " Synset('coupe.n.01'),\n",
       " Synset('cruiser.n.01'),\n",
       " Synset('electric.n.01'),\n",
       " Synset('gas_guzzler.n.01'),\n",
       " Synset('hardtop.n.01'),\n",
       " Synset('hatchback.n.01'),\n",
       " Synset('horseless_carriage.n.01'),\n",
       " Synset('hot_rod.n.01'),\n",
       " Synset('jeep.n.01'),\n",
       " Synset('limousine.n.01'),\n",
       " Synset('loaner.n.02'),\n",
       " Synset('minicar.n.01'),\n",
       " Synset('minivan.n.01'),\n",
       " Synset('model_t.n.01'),\n",
       " Synset('pace_car.n.01'),\n",
       " Synset('racer.n.02'),\n",
       " Synset('roadster.n.01'),\n",
       " Synset('sedan.n.01'),\n",
       " Synset('sport_utility.n.01'),\n",
       " Synset('sports_car.n.01'),\n",
       " Synset('stanley_steamer.n.01'),\n",
       " Synset('stock_car.n.01'),\n",
       " Synset('subcompact.n.01'),\n",
       " Synset('touring_car.n.01'),\n",
       " Synset('used-car.n.01')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('motorcar.n.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 部分整体\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('long.a.01.long')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 反义词\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.lemma('short.a.01.short').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'unicode_repr',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "dir(wn.synset('beautiful.a.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syn=wordnet.synsets('motorcar')[0]\n",
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brave', 'courageous']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('brave.a.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fc2a722b2e83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'brave'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m         \u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('brave').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'possessing or displaying courage; able to face and deal with danger or fear without flinching'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('brave.a.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn.synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_all_hypernyms',\n",
       " '_definition',\n",
       " '_examples',\n",
       " '_frame_ids',\n",
       " '_hypernyms',\n",
       " '_instance_hypernyms',\n",
       " '_iter_hypernym_lists',\n",
       " '_lemma_names',\n",
       " '_lemma_pointers',\n",
       " '_lemmas',\n",
       " '_lexname',\n",
       " '_max_depth',\n",
       " '_min_depth',\n",
       " '_name',\n",
       " '_needs_root',\n",
       " '_offset',\n",
       " '_pointers',\n",
       " '_pos',\n",
       " '_related',\n",
       " '_shortest_hypernym_paths',\n",
       " '_wordnet_corpus_reader',\n",
       " 'also_sees',\n",
       " 'attributes',\n",
       " 'causes',\n",
       " 'closure',\n",
       " 'common_hypernyms',\n",
       " 'definition',\n",
       " 'entailments',\n",
       " 'examples',\n",
       " 'frame_ids',\n",
       " 'hypernym_distances',\n",
       " 'hypernym_paths',\n",
       " 'hypernyms',\n",
       " 'hyponyms',\n",
       " 'instance_hypernyms',\n",
       " 'instance_hyponyms',\n",
       " 'jcn_similarity',\n",
       " 'lch_similarity',\n",
       " 'lemma_names',\n",
       " 'lemmas',\n",
       " 'lexname',\n",
       " 'lin_similarity',\n",
       " 'lowest_common_hypernyms',\n",
       " 'max_depth',\n",
       " 'member_holonyms',\n",
       " 'member_meronyms',\n",
       " 'min_depth',\n",
       " 'name',\n",
       " 'offset',\n",
       " 'part_holonyms',\n",
       " 'part_meronyms',\n",
       " 'path_similarity',\n",
       " 'pos',\n",
       " 'region_domains',\n",
       " 'res_similarity',\n",
       " 'root_hypernyms',\n",
       " 'shortest_path_distance',\n",
       " 'similar_tos',\n",
       " 'substance_holonyms',\n",
       " 'substance_meronyms',\n",
       " 'topic_domains',\n",
       " 'tree',\n",
       " 'unicode_repr',\n",
       " 'usage_domains',\n",
       " 'verb_groups',\n",
       " 'wup_similarity']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(wn.synset('brave.a.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculating wordnet synset similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Synset.definition of Synset('right_whale.n.01')>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('right_whale.n.01').definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"large Arctic whalebone whale; allegedly the `right' whale to hunt because of its valuable whalebone and oil\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synset('right_whale.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'small finback of coastal waters of Atlantic and Pacific'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('minke_whale.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('giant.n.04.giant'),\n",
       " Lemma('giant.n.04.hulk'),\n",
       " Lemma('giant.n.04.heavyweight'),\n",
       " Lemma('giant.n.04.whale')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whale.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'lemmas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f1a92497ec3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'lemmas'"
     ]
    }
   ],
   "source": [
    "help(wn.synset.lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method lemmas in module nltk.corpus.reader.wordnet:\n",
      "\n",
      "lemmas(lang='eng') method of nltk.corpus.reader.wordnet.Synset instance\n",
      "    Return all the lemma objects associated with the synset\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wn.synset('whale.n.01').lemmas) #lemma词根、词元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('check.v.06'), Synset('check.v.22'), Synset('see.v.10')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('verify.v.01').hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'confirm the truth of'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('verify.v.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'check or regulate (a scientific experiment) by conducting a parallel experiment or comparing with another standard'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('verify.v.02').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-34dfd1434395>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'verify'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m         \u001b[1;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1229\u001b[1;33m         \u001b[0mlemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1230\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "wn.synset('verify').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "right.path_similarity(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "g=wn.synset('big.a.01')\n",
    "h=wn.synset('large.a.01')\n",
    "print (g.path_similarity(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058823529411764705"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=wn.synset('apple.n.01')\n",
    "b=wn.synset('bear.n.01')\n",
    "a.path_similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=wn.synset('apple.n.01')\n",
    "b=wn.synset('bear.n.01')\n",
    "a.wup_similarity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=wn.synset('fruit.n.01')\n",
    "a.path_similarity(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=wn.synset('fruit.n.01')\n",
    "a.wup_similarity(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=wn.synset('notebook.n.01')\n",
    "ib=wn.synset('instruction_book.n.01')\n",
    "nb.wup_similarity(ib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discovering word collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object bigrams at 0x0000029CA4615B48>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "a=\"Beijing Language and Culture University\"\n",
    "tokens=a.split()\n",
    "bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Beijing', 'Language', 'and', 'Culture', 'University']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "a=\"Beijing Language and Culture University\"\n",
    "tokens=a.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object bigrams at 0x000001F70B6D5888>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beijing', 'Language'),\n",
       " ('Language', 'and'),\n",
       " ('and', 'Culture'),\n",
       " ('Culture', 'University')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tokens)) #这里必须要转化成list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist1=FreqDist(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 19317 samples and 260819 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print (fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'palmy': 3,\n",
       "          'ditto': 1,\n",
       "          'trace': 5,\n",
       "          'shrunk': 1,\n",
       "          'swamped': 1,\n",
       "          '1840': 1,\n",
       "          'relied': 1,\n",
       "          'Socratic': 1,\n",
       "          'insanity': 6,\n",
       "          'crackled': 1,\n",
       "          'tonic': 1,\n",
       "          'chronically': 1,\n",
       "          'Knights': 2,\n",
       "          'terse': 1,\n",
       "          'BACK': 5,\n",
       "          'folds': 6,\n",
       "          'tidiness': 1,\n",
       "          'needs': 20,\n",
       "          'journeyman': 1,\n",
       "          'bay': 4,\n",
       "          'accounted': 9,\n",
       "          'listened': 1,\n",
       "          'Gods': 3,\n",
       "          'compassion': 1,\n",
       "          'frontier': 1,\n",
       "          'cleats': 1,\n",
       "          'enlisting': 1,\n",
       "          'ruddy': 3,\n",
       "          'temperate': 3,\n",
       "          'laughing': 4,\n",
       "          'floated': 18,\n",
       "          'jointed': 1,\n",
       "          'playful': 3,\n",
       "          'confabulations': 1,\n",
       "          'religionists': 1,\n",
       "          'expressions': 2,\n",
       "          'eider': 1,\n",
       "          'muttered': 17,\n",
       "          'fiery': 23,\n",
       "          'woody': 1,\n",
       "          'peering': 7,\n",
       "          'sands': 2,\n",
       "          'stunsail': 3,\n",
       "          'prophet': 13,\n",
       "          'jungle': 1,\n",
       "          'knocking': 7,\n",
       "          'labors': 2,\n",
       "          'Lucifer': 1,\n",
       "          'practical': 13,\n",
       "          'congregated': 2,\n",
       "          '84': 1,\n",
       "          'palmed': 2,\n",
       "          'leakage': 1,\n",
       "          'exasperations': 1,\n",
       "          'history': 15,\n",
       "          'took': 49,\n",
       "          'sent': 16,\n",
       "          'LIGHTNING': 1,\n",
       "          'gruff': 1,\n",
       "          'slain': 12,\n",
       "          'paws': 2,\n",
       "          'monstrousest': 1,\n",
       "          'artist': 4,\n",
       "          'handles': 3,\n",
       "          'case': 69,\n",
       "          'fanning': 2,\n",
       "          'wayward': 1,\n",
       "          'running': 44,\n",
       "          'misty': 3,\n",
       "          'Abraham': 3,\n",
       "          'lunar': 1,\n",
       "          'bison': 3,\n",
       "          'scrabble': 1,\n",
       "          'MAT': 1,\n",
       "          'governor': 5,\n",
       "          'smoothe': 3,\n",
       "          'laughable': 1,\n",
       "          'swear': 13,\n",
       "          'cashier': 1,\n",
       "          'Mounttop': 4,\n",
       "          'damage': 1,\n",
       "          'budge': 4,\n",
       "          'sideways': 27,\n",
       "          'animosity': 1,\n",
       "          'laughter': 1,\n",
       "          'visiting': 3,\n",
       "          'Dragged': 1,\n",
       "          'Golden': 3,\n",
       "          'tolerably': 2,\n",
       "          'Whilst': 1,\n",
       "          'planned': 1,\n",
       "          'vowing': 1,\n",
       "          'visage': 1,\n",
       "          'Carrol': 1,\n",
       "          'Aunt': 4,\n",
       "          'impressions': 10,\n",
       "          'resident': 1,\n",
       "          'headlands': 2,\n",
       "          'weighty': 3,\n",
       "          'Pan': 2,\n",
       "          'mathematically': 1,\n",
       "          'reduced': 2,\n",
       "          'democrat': 1,\n",
       "          'extraordinary': 7,\n",
       "          'tiller': 13,\n",
       "          'kissed': 1,\n",
       "          'proceeded': 6,\n",
       "          'lieu': 2,\n",
       "          \":--'\": 2,\n",
       "          'Hussey': 16,\n",
       "          'indecorous': 1,\n",
       "          'Daggoo': 34,\n",
       "          'infidel': 7,\n",
       "          'BREAKWATER': 1,\n",
       "          'glutinous': 1,\n",
       "          'encouraged': 1,\n",
       "          'windpipe': 2,\n",
       "          'sounds': 12,\n",
       "          'rocky': 4,\n",
       "          'Joe': 1,\n",
       "          'constituents': 1,\n",
       "          'position': 15,\n",
       "          'ripe': 6,\n",
       "          'clotting': 1,\n",
       "          'GRAND': 2,\n",
       "          'proas': 1,\n",
       "          'gateways': 1,\n",
       "          'seal': 6,\n",
       "          'Law': 1,\n",
       "          'terrible': 21,\n",
       "          'strips': 2,\n",
       "          'spires': 6,\n",
       "          'operas': 1,\n",
       "          'drench': 1,\n",
       "          'weepons': 1,\n",
       "          'shadiest': 1,\n",
       "          'provincials': 1,\n",
       "          'being': 219,\n",
       "          'downcast': 2,\n",
       "          'bounce': 2,\n",
       "          'Pantheon': 1,\n",
       "          'countries': 4,\n",
       "          'blaze': 2,\n",
       "          'humbly': 2,\n",
       "          'weld': 2,\n",
       "          'rested': 2,\n",
       "          'awaited': 2,\n",
       "          'eyelids': 1,\n",
       "          'LASCAR': 1,\n",
       "          'applying': 1,\n",
       "          'tipping': 1,\n",
       "          'ought': 8,\n",
       "          'carving': 6,\n",
       "          'Above': 2,\n",
       "          'pork': 2,\n",
       "          'reading': 8,\n",
       "          'scrupulously': 1,\n",
       "          'unpleasing': 1,\n",
       "          'resentment': 2,\n",
       "          'hustings': 1,\n",
       "          '1750': 1,\n",
       "          'needlessly': 1,\n",
       "          'poniards': 1,\n",
       "          'driver': 4,\n",
       "          'joints': 5,\n",
       "          'SCATTER': 1,\n",
       "          'self': 25,\n",
       "          'greybeards': 1,\n",
       "          'merest': 1,\n",
       "          'Witness': 2,\n",
       "          'language': 6,\n",
       "          'forbear': 3,\n",
       "          'listlessness': 1,\n",
       "          'enforced': 1,\n",
       "          'Societies': 1,\n",
       "          'moved': 14,\n",
       "          'edge': 7,\n",
       "          'gurgling': 3,\n",
       "          'acquired': 2,\n",
       "          'celestial': 5,\n",
       "          'groans': 1,\n",
       "          'gouge': 2,\n",
       "          'stop': 25,\n",
       "          'phenomena': 1,\n",
       "          'ain': 19,\n",
       "          'watchmakers': 1,\n",
       "          'Didst': 1,\n",
       "          'surrounds': 2,\n",
       "          'crossed': 13,\n",
       "          'printed': 2,\n",
       "          'ENSUING': 1,\n",
       "          'wakefulness': 1,\n",
       "          'comprehensive': 2,\n",
       "          'throw': 13,\n",
       "          'schoolmaster': 6,\n",
       "          'baskets': 1,\n",
       "          'viciousness': 2,\n",
       "          'fishery': 54,\n",
       "          'astrological': 1,\n",
       "          'brats': 1,\n",
       "          'gums': 2,\n",
       "          'drained': 1,\n",
       "          'Gabriel': 20,\n",
       "          'altitudes': 2,\n",
       "          'mariner': 16,\n",
       "          'clanking': 2,\n",
       "          'scrubbed': 1,\n",
       "          'spoken': 13,\n",
       "          'livelong': 2,\n",
       "          'undiscoverable': 1,\n",
       "          'comforting': 1,\n",
       "          'prairies': 4,\n",
       "          'reverenced': 1,\n",
       "          'essayed': 1,\n",
       "          'slanderous': 1,\n",
       "          'trucks': 4,\n",
       "          'waited': 1,\n",
       "          'tragic': 4,\n",
       "          'NEW': 2,\n",
       "          'standing': 73,\n",
       "          'Algerine': 1,\n",
       "          'infinitely': 4,\n",
       "          'supported': 2,\n",
       "          'bending': 2,\n",
       "          'Hygiene': 1,\n",
       "          'hereupon': 2,\n",
       "          'scrolls': 1,\n",
       "          'Sag': 8,\n",
       "          'whaleships': 1,\n",
       "          'valor': 2,\n",
       "          'Justice': 2,\n",
       "          'becomes': 12,\n",
       "          'superstitious': 10,\n",
       "          'true': 73,\n",
       "          'turmoil': 1,\n",
       "          'discretion': 2,\n",
       "          'thirty': 26,\n",
       "          'kinds': 3,\n",
       "          'overlapping': 2,\n",
       "          'jeopardize': 1,\n",
       "          'treble': 1,\n",
       "          'zones': 3,\n",
       "          '---,': 1,\n",
       "          'CHEERLY': 1,\n",
       "          'conveniently': 1,\n",
       "          'Babel': 2,\n",
       "          'rip': 1,\n",
       "          'toughest': 1,\n",
       "          'sorry': 6,\n",
       "          'consolation': 1,\n",
       "          'unfulfilments': 1,\n",
       "          'SHIPWRECK': 2,\n",
       "          'swims': 8,\n",
       "          'noble': 44,\n",
       "          'officers': 20,\n",
       "          'jumped': 10,\n",
       "          'available': 2,\n",
       "          'avenues': 3,\n",
       "          'frankly': 3,\n",
       "          'currents': 7,\n",
       "          'wedged': 3,\n",
       "          'shrink': 2,\n",
       "          'deceitfulness': 2,\n",
       "          'tracings': 1,\n",
       "          'shadowed': 2,\n",
       "          'congratulate': 1,\n",
       "          'contended': 1,\n",
       "          'extorting': 1,\n",
       "          'salutations': 2,\n",
       "          'countersinkers': 1,\n",
       "          'Overhearing': 2,\n",
       "          'balancing': 3,\n",
       "          'teeter': 1,\n",
       "          'chaplain': 3,\n",
       "          'spiracles': 1,\n",
       "          'For': 197,\n",
       "          'ebon': 1,\n",
       "          'incontinently': 1,\n",
       "          'grove': 3,\n",
       "          'marge': 1,\n",
       "          'snored': 1,\n",
       "          'heat': 16,\n",
       "          'gabled': 1,\n",
       "          'oversee': 1,\n",
       "          'BANKS': 1,\n",
       "          'Castle': 2,\n",
       "          'intrantem': 1,\n",
       "          'alike': 10,\n",
       "          'bedstead': 2,\n",
       "          'coolness': 2,\n",
       "          'tangle': 1,\n",
       "          'incorporated': 2,\n",
       "          'Alps': 3,\n",
       "          'destinations': 1,\n",
       "          'dropping': 16,\n",
       "          'inquiring': 5,\n",
       "          'Tistig': 2,\n",
       "          'erected': 5,\n",
       "          'answers': 3,\n",
       "          'fenced': 1,\n",
       "          'antecedent': 1,\n",
       "          'expansive': 2,\n",
       "          'wantonly': 1,\n",
       "          'witch': 1,\n",
       "          'RICHARD': 1,\n",
       "          'prick': 6,\n",
       "          'unblinkingly': 1,\n",
       "          'growl': 2,\n",
       "          'roundness': 1,\n",
       "          'MALTESE': 2,\n",
       "          'timber': 6,\n",
       "          'pose': 1,\n",
       "          'Milky': 1,\n",
       "          'doing': 15,\n",
       "          'pedlar': 1,\n",
       "          'ex': 1,\n",
       "          'transcends': 1,\n",
       "          'affrighted': 5,\n",
       "          'deduction': 1,\n",
       "          'VICE': 3,\n",
       "          'commence': 2,\n",
       "          'southern': 5,\n",
       "          'unobserved': 2,\n",
       "          'quills': 2,\n",
       "          'soldier': 1,\n",
       "          'muffins': 1,\n",
       "          'Tahitian': 3,\n",
       "          'digesting': 2,\n",
       "          'cooked': 6,\n",
       "          'loathed': 1,\n",
       "          'sail': 96,\n",
       "          'zoned': 1,\n",
       "          'colour': 16,\n",
       "          'lastly': 2,\n",
       "          'subdivisible': 1,\n",
       "          'consisted': 2,\n",
       "          'invert': 1,\n",
       "          'gor': 1,\n",
       "          'swerve': 4,\n",
       "          'unensanguined': 1,\n",
       "          'treachery': 1,\n",
       "          'octavo': 1,\n",
       "          'turning': 52,\n",
       "          'bridges': 2,\n",
       "          'bottomless': 7,\n",
       "          'wat': 1,\n",
       "          'patris': 1,\n",
       "          'marvels': 8,\n",
       "          'Paracelsus': 1,\n",
       "          'sleeps': 7,\n",
       "          'musket': 7,\n",
       "          'clearer': 1,\n",
       "          'phospher': 1,\n",
       "          'offered': 12,\n",
       "          'flowing': 4,\n",
       "          'Whole': 1,\n",
       "          'indifferent': 11,\n",
       "          'nowadays': 5,\n",
       "          'vagabond': 2,\n",
       "          'masculine': 1,\n",
       "          'incidental': 1,\n",
       "          'green': 50,\n",
       "          'Anyway': 1,\n",
       "          'oaken': 5,\n",
       "          'unnecessary': 6,\n",
       "          'Carthage': 1,\n",
       "          'maddening': 1,\n",
       "          'bellows': 1,\n",
       "          'disobedience': 2,\n",
       "          'LEAPS': 1,\n",
       "          'Krusensterns': 1,\n",
       "          'sharppointed': 1,\n",
       "          'vigilance': 7,\n",
       "          'deliverer': 1,\n",
       "          'Push': 1,\n",
       "          '26': 1,\n",
       "          'rally': 2,\n",
       "          'reciprocal': 1,\n",
       "          'exclamation': 5,\n",
       "          'visibly': 2,\n",
       "          'victory': 1,\n",
       "          'ruins': 2,\n",
       "          'alluding': 2,\n",
       "          'pick': 11,\n",
       "          'tick': 1,\n",
       "          'deaths': 1,\n",
       "          'blossom': 1,\n",
       "          'deserving': 1,\n",
       "          'Line': 16,\n",
       "          'dawn': 9,\n",
       "          'outran': 1,\n",
       "          'Et': 1,\n",
       "          'incuriously': 1,\n",
       "          'Scandinavian': 2,\n",
       "          'invertedly': 1,\n",
       "          'enslaved': 1,\n",
       "          'skirted': 2,\n",
       "          'smiting': 6,\n",
       "          'mastheads': 1,\n",
       "          'John': 11,\n",
       "          'initials': 1,\n",
       "          'necessitates': 3,\n",
       "          'successively': 1,\n",
       "          'tablet': 1,\n",
       "          'experimental': 1,\n",
       "          'rises': 7,\n",
       "          'Cut': 3,\n",
       "          'flour': 1,\n",
       "          'sittin': 1,\n",
       "          'fields': 7,\n",
       "          'Landlord': 7,\n",
       "          'soulless': 1,\n",
       "          'cloistered': 1,\n",
       "          'provides': 2,\n",
       "          'ray': 2,\n",
       "          'incarnation': 2,\n",
       "          'pointless': 1,\n",
       "          'Republican': 1,\n",
       "          'sanguinary': 1,\n",
       "          'indirectly': 13,\n",
       "          'late': 29,\n",
       "          'draggingly': 1,\n",
       "          'breezeless': 1,\n",
       "          'tallow': 4,\n",
       "          'riveted': 5,\n",
       "          'droopings': 1,\n",
       "          'Spaniards': 2,\n",
       "          'complexioned': 2,\n",
       "          'boasted': 3,\n",
       "          'universally': 4,\n",
       "          'heavy': 37,\n",
       "          'completely': 33,\n",
       "          'volatile': 1,\n",
       "          'bade': 5,\n",
       "          'paddle': 4,\n",
       "          'borne': 9,\n",
       "          'grocers': 1,\n",
       "          'communicating': 2,\n",
       "          'advance': 21,\n",
       "          'miser': 2,\n",
       "          'gentlemanlike': 2,\n",
       "          'Mansoul': 1,\n",
       "          'hailed': 21,\n",
       "          'weakling': 1,\n",
       "          'remonstrance': 1,\n",
       "          'mediocrity': 1,\n",
       "          'timberheads': 1,\n",
       "          'signifies': 2,\n",
       "          'dramatically': 1,\n",
       "          'EDMUND': 2,\n",
       "          '8': 1,\n",
       "          'happen': 8,\n",
       "          'Starbuck': 196,\n",
       "          'hesitatingly': 1,\n",
       "          'balmed': 1,\n",
       "          'Khan': 1,\n",
       "          'brains': 8,\n",
       "          'poncho': 1,\n",
       "          '64': 1,\n",
       "          'LONG': 4,\n",
       "          'passionlessness': 1,\n",
       "          'damply': 1,\n",
       "          'Lackaday': 1,\n",
       "          'risks': 1,\n",
       "          'down': 364,\n",
       "          'Voyages': 3,\n",
       "          'torrid': 2,\n",
       "          'portion': 4,\n",
       "          'nailed': 11,\n",
       "          'commandment': 1,\n",
       "          'maples': 1,\n",
       "          'throb': 2,\n",
       "          'stripped': 10,\n",
       "          'BEWARE': 1,\n",
       "          'spurrings': 1,\n",
       "          'dubious': 3,\n",
       "          'thicker': 1,\n",
       "          'scamp': 1,\n",
       "          'sheaves': 3,\n",
       "          'Scorpion': 1,\n",
       "          'gleam': 1,\n",
       "          'peels': 2,\n",
       "          'interesting': 8,\n",
       "          'ALL': 9,\n",
       "          'sir': 143,\n",
       "          'gasped': 1,\n",
       "          'girlish': 1,\n",
       "          'perpendicularly': 8,\n",
       "          'toils': 1,\n",
       "          'vicissitude': 1,\n",
       "          'loaf': 1,\n",
       "          'respiration': 2,\n",
       "          'blew': 8,\n",
       "          'care': 14,\n",
       "          'melodious': 1,\n",
       "          'glorying': 1,\n",
       "          'lance': 45,\n",
       "          'views': 3,\n",
       "          'what': 442,\n",
       "          'orbs': 2,\n",
       "          'leaves': 19,\n",
       "          'slits': 1,\n",
       "          'consequences': 2,\n",
       "          'RAZOR': 2,\n",
       "          'bruised': 1,\n",
       "          'exercises': 1,\n",
       "          'properties': 1,\n",
       "          'nurse': 3,\n",
       "          'flinty': 1,\n",
       "          'genteelly': 1,\n",
       "          'placidity': 2,\n",
       "          'fasten': 5,\n",
       "          'crowned': 3,\n",
       "          'toss': 7,\n",
       "          'partiality': 1,\n",
       "          'snap': 7,\n",
       "          'wring': 1,\n",
       "          'spectre': 1,\n",
       "          'walk': 8,\n",
       "          'person': 28,\n",
       "          'waxes': 1,\n",
       "          'Cross': 3,\n",
       "          'diametrically': 1,\n",
       "          'Scotch': 3,\n",
       "          'July': 2,\n",
       "          'canoe': 11,\n",
       "          'unrelenting': 1,\n",
       "          'surest': 1,\n",
       "          'solecism': 1,\n",
       "          'screwed': 7,\n",
       "          'hypothetically': 2,\n",
       "          'wrought': 6,\n",
       "          'colonnades': 2,\n",
       "          'Rope': 2,\n",
       "          'asses': 1,\n",
       "          'wonderingly': 1,\n",
       "          'ge': 1,\n",
       "          'evangelist': 2,\n",
       "          'beggars': 2,\n",
       "          'comforts': 1,\n",
       "          'chowders': 3,\n",
       "          'satiety': 1,\n",
       "          'unsplinterable': 1,\n",
       "          'flattering': 2,\n",
       "          'disinfecting': 1,\n",
       "          'essential': 3,\n",
       "          'judges': 1,\n",
       "          'Whaleman': 1,\n",
       "          'Buoy': 1,\n",
       "          'Swain': 1,\n",
       "          'keeper': 1,\n",
       "          'tarpaulin': 2,\n",
       "          'unassailable': 1,\n",
       "          'Blacksmith': 1,\n",
       "          '(': 210,\n",
       "          'warrantry': 1,\n",
       "          'conical': 1,\n",
       "          'decoration': 1,\n",
       "          'Dr': 6,\n",
       "          'gain': 13,\n",
       "          'withhold': 2,\n",
       "          'blooded': 2,\n",
       "          'seats': 2,\n",
       "          'aliment': 1,\n",
       "          'fasting': 3,\n",
       "          'Huggins': 2,\n",
       "          'eel': 2,\n",
       "          'retains': 1,\n",
       "          'welding': 1,\n",
       "          'wines': 1,\n",
       "          'Several': 1,\n",
       "          'pours': 2,\n",
       "          'Voyage': 3,\n",
       "          'swore': 7,\n",
       "          'medal': 1,\n",
       "          'light': 77,\n",
       "          'sort': 152,\n",
       "          'vortex': 2,\n",
       "          'venting': 1,\n",
       "          'Ahasuerus': 2,\n",
       "          'nose': 31,\n",
       "          'proverb': 4,\n",
       "          'Dut': 1,\n",
       "          'promising': 4,\n",
       "          'tauntings': 1,\n",
       "          'cabled': 1,\n",
       "          'Northman': 1,\n",
       "          'irregular': 6,\n",
       "          'remonstrating': 1,\n",
       "          'consideration': 8,\n",
       "          'Carefully': 1,\n",
       "          'silent': 25,\n",
       "          'sympathetical': 1,\n",
       "          'PARLIAMENT': 1,\n",
       "          'wallows': 1,\n",
       "          'veracity': 1,\n",
       "          'slouching': 2,\n",
       "          'Baling': 1,\n",
       "          'Immortal': 1,\n",
       "          'justice': 2,\n",
       "          'clinking': 1,\n",
       "          'bag': 21,\n",
       "          'diverged': 3,\n",
       "          'nominally': 4,\n",
       "          'SISTER': 1,\n",
       "          'y': 2,\n",
       "          'shadow': 15,\n",
       "          'aghast': 4,\n",
       "          'Dives': 3,\n",
       "          'bury': 6,\n",
       "          'cellar': 2,\n",
       "          'clad': 1,\n",
       "          'olden': 1,\n",
       "          'buttressed': 1,\n",
       "          'recent': 2,\n",
       "          'majestical': 1,\n",
       "          'trifles': 2,\n",
       "          'unachieved': 1,\n",
       "          'special': 15,\n",
       "          'feelingly': 2,\n",
       "          'Faintly': 1,\n",
       "          'text': 4,\n",
       "          'troubled': 15,\n",
       "          'foothold': 1,\n",
       "          'drags': 4,\n",
       "          'swaths': 2,\n",
       "          'inveterate': 1,\n",
       "          'bodied': 1,\n",
       "          'central': 11,\n",
       "          'forsaken': 1,\n",
       "          'Zodiac': 2,\n",
       "          'HEARS': 1,\n",
       "          'sartin': 1,\n",
       "          'acquainted': 4,\n",
       "          'solitaries': 2,\n",
       "          'Must': 1,\n",
       "          'impregnated': 1,\n",
       "          'observable': 2,\n",
       "          'generic': 4,\n",
       "          'deliberated': 1,\n",
       "          'shy': 1,\n",
       "          'affluent': 1,\n",
       "          'sanctuary': 1,\n",
       "          'clouds': 12,\n",
       "          'ANCHORS': 1,\n",
       "          'drifting': 3,\n",
       "          'javelin': 2,\n",
       "          'Even': 10,\n",
       "          'deplore': 1,\n",
       "          'sunset': 8,\n",
       "          'Circassian': 1,\n",
       "          'ascribe': 4,\n",
       "          'snakes': 1,\n",
       "          'pendulum': 1,\n",
       "          'cursing': 1,\n",
       "          'two': 285,\n",
       "          'lends': 1,\n",
       "          'landed': 12,\n",
       "          'bodily': 26,\n",
       "          'vainly': 6,\n",
       "          'adopting': 1,\n",
       "          'bedroom': 1,\n",
       "          'Wondrous': 1,\n",
       "          'Negro': 1,\n",
       "          'rise': 20,\n",
       "          'miller': 1,\n",
       "          'disorderly': 1,\n",
       "          'hatches': 10,\n",
       "          'skrimshandering': 1,\n",
       "          'hitherto': 16,\n",
       "          'misanthropic': 2,\n",
       "          'wrangling': 1,\n",
       "          'ewer': 1,\n",
       "          'malignantly': 1,\n",
       "          'sharply': 3,\n",
       "          'Fuego': 1,\n",
       "          'humane': 3,\n",
       "          'Whalebone': 2,\n",
       "          'brooks': 2,\n",
       "          'College': 2,\n",
       "          'royalty': 3,\n",
       "          'Flask': 104,\n",
       "          'PREFACE': 1,\n",
       "          'Little': 2,\n",
       "          'Whisper': 1,\n",
       "          'indiscretions': 1,\n",
       "          'Sphinx': 1,\n",
       "          'adjusting': 1,\n",
       "          'cave': 5,\n",
       "          'dare': 15,\n",
       "          'minor': 1,\n",
       "          'trepidation': 2,\n",
       "          'e': 24,\n",
       "          'heavenly': 6,\n",
       "          'lest': 7,\n",
       "          'crammed': 1,\n",
       "          'victor': 3,\n",
       "          'Eddystone': 2,\n",
       "          'reservation': 1,\n",
       "          'introduction': 1,\n",
       "          'dazzling': 3,\n",
       "          'whitish': 2,\n",
       "          'Perth': 18,\n",
       "          'thrashing': 1,\n",
       "          'savages': 10,\n",
       "          'rarely': 2,\n",
       "          'Muezzin': 1,\n",
       "          'connecting': 3,\n",
       "          'horse': 26,\n",
       "          'tearingly': 1,\n",
       "          'thread': 2,\n",
       "          'raise': 8,\n",
       "          'monotonously': 1,\n",
       "          'investigated': 1,\n",
       "          'choice': 5,\n",
       "          'undeterred': 1,\n",
       "          'Darkness': 1,\n",
       "          'ragamuffin': 1,\n",
       "          'stilts': 1,\n",
       "          'Lane': 1,\n",
       "          'unusual': 12,\n",
       "          'divine': 6,\n",
       "          'profounder': 2,\n",
       "          'companionway': 1,\n",
       "          'smugglers': 1,\n",
       "          'mistifying': 2,\n",
       "          'Brother': 1,\n",
       "          'potion': 1,\n",
       "          'mediums': 1,\n",
       "          'beards': 3,\n",
       "          'unaccountably': 2,\n",
       "          'fugitive': 6,\n",
       "          'tantalizing': 3,\n",
       "          'hooroosh': 1,\n",
       "          'Cuvier': 10,\n",
       "          'worked': 8,\n",
       "          'moose': 2,\n",
       "          'devilish': 9,\n",
       "          'marquee': 1,\n",
       "          'restraint': 1,\n",
       "          'untrackably': 1,\n",
       "          'WHICH': 2,\n",
       "          'saint': 2,\n",
       "          'executioner': 1,\n",
       "          'WHALING': 8,\n",
       "          'handing': 3,\n",
       "          'smoker': 1,\n",
       "          'representing': 7,\n",
       "          'rendered': 1,\n",
       "          'BROTHER': 1,\n",
       "          'Dish': 1,\n",
       "          'Quakeress': 2,\n",
       "          'either': 39,\n",
       "          'three': 237,\n",
       "          'fatalities': 1,\n",
       "          'rarities': 1,\n",
       "          'builders': 2,\n",
       "          'Begone': 3,\n",
       "          'jealous': 2,\n",
       "          'boldness': 2,\n",
       "          'condemning': 2,\n",
       "          'investment': 1,\n",
       "          'M': 1,\n",
       "          'brilliancy': 3,\n",
       "          'building': 2,\n",
       "          'finite': 1,\n",
       "          'Traitors': 1,\n",
       "          'remind': 1,\n",
       "          'oh': 36,\n",
       "          'traits': 2,\n",
       "          'birthmark': 1,\n",
       "          'lines': 37,\n",
       "          'acquiescence': 1,\n",
       "          'Coast': 2,\n",
       "          'marlingspike': 2,\n",
       "          'baron': 1,\n",
       "          'lotions': 1,\n",
       "          'farewell': 2,\n",
       "          'embellishments': 1,\n",
       "          'glitteringly': 1,\n",
       "          'pinioned': 3,\n",
       "          'Newfoundland': 2,\n",
       "          'Bishop': 5,\n",
       "          'Headers': 1,\n",
       "          'discovered': 10,\n",
       "          'concern': 10,\n",
       "          'obvious': 11,\n",
       "          'interval': 25,\n",
       "          'tropic': 4,\n",
       "          'uninterpenetratingly': 1,\n",
       "          'Paint': 1,\n",
       "          'innate': 1,\n",
       "          'frosts': 2,\n",
       "          'Victory': 1,\n",
       "          'viol': 1,\n",
       "          'telegraph': 1,\n",
       "          'scratched': 1,\n",
       "          'historian': 3,\n",
       "          'babes': 1,\n",
       "          'glue': 2,\n",
       "          'dungeoned': 1,\n",
       "          'insomuch': 2,\n",
       "          'Horned': 1,\n",
       "          \";--'\": 1,\n",
       "          'Unhinge': 1,\n",
       "          '122': 1,\n",
       "          'Ready': 2,\n",
       "          'WATCH': 1,\n",
       "          'unmoor': 1,\n",
       "          'cloth': 8,\n",
       "          'mentions': 1,\n",
       "          'minds': 14,\n",
       "          'drugg': 3,\n",
       "          'barnacle': 1,\n",
       "          'wakened': 2,\n",
       "          'fared': 1,\n",
       "          'jewel': 2,\n",
       "          'properly': 9,\n",
       "          'apricot': 1,\n",
       "          'Ram': 3,\n",
       "          'employments': 2,\n",
       "          'serious': 9,\n",
       "          'gait': 2,\n",
       "          'persisting': 2,\n",
       "          'sallow': 1,\n",
       "          'since': 63,\n",
       "          'parallels': 1,\n",
       "          'Tennessee': 1,\n",
       "          'RICHARDSON': 1,\n",
       "          'hereditarily': 2,\n",
       "          'requires': 7,\n",
       "          'cases': 25,\n",
       "          'shudderings': 2,\n",
       "          'invisible': 15,\n",
       "          'intelligently': 1,\n",
       "          'shrouded': 6,\n",
       "          'shades': 6,\n",
       "          'buttoning': 2,\n",
       "          'unsays': 1,\n",
       "          'paid': 7,\n",
       "          'guardian': 1,\n",
       "          'stamped': 4,\n",
       "          'ducking': 4,\n",
       "          'pushed': 15,\n",
       "          'pecks': 3,\n",
       "          'endangered': 3,\n",
       "          'wanted': 9,\n",
       "          'perished': 6,\n",
       "          'crossing': 16,\n",
       "          'analysis': 1,\n",
       "          'presents': 7,\n",
       "          'chancery': 1,\n",
       "          'error': 3,\n",
       "          'Wood': 2,\n",
       "          'mask': 3,\n",
       "          '6': 1,\n",
       "          'cylinders': 1,\n",
       "          'deny': 4,\n",
       "          'dividends': 1,\n",
       "          'toad': 1,\n",
       "          'rend': 1,\n",
       "          'L150': 1,\n",
       "          'weazel': 1,\n",
       "          'lanes': 1,\n",
       "          'larders': 1,\n",
       "          'loan': 1,\n",
       "          'feathery': 1,\n",
       "          'expression': 9,\n",
       "          'villages': 4,\n",
       "          'sacrifice': 2,\n",
       "          'Great': 13,\n",
       "          'motley': 1,\n",
       "          'obliterated': 1,\n",
       "          'Polynesia': 1,\n",
       "          'seen': 161,\n",
       "          'purposed': 1,\n",
       "          'sailors': 50,\n",
       "          'hole': 44,\n",
       "          'fragrance': 1,\n",
       "          'methought': 1,\n",
       "          'mischievous': 1,\n",
       "          'Convulsively': 1,\n",
       "          'descry': 1,\n",
       "          'muffledness': 1,\n",
       "          'New': 47,\n",
       "          '890': 1,\n",
       "          'quilt': 2,\n",
       "          'remarkable': 13,\n",
       "          'contemptible': 4,\n",
       "          'profanely': 2,\n",
       "          'hover': 2,\n",
       "          'pain': 2,\n",
       "          'scout': 1,\n",
       "          'piers': 1,\n",
       "          'durable': 1,\n",
       "          'weariest': 1,\n",
       "          'spiralizes': 1,\n",
       "          'fleece': 1,\n",
       "          'exactitude': 1,\n",
       "          'wake': 40,\n",
       "          'jingling': 2,\n",
       "          'sundry': 3,\n",
       "          'crying': 3,\n",
       "          'ones': 19,\n",
       "          'playfully': 1,\n",
       "          'shutter': 1,\n",
       "          'Plate': 1,\n",
       "          'coolly': 7,\n",
       "          'Tail': 1,\n",
       "          'bejuggled': 1,\n",
       "          'return': 13,\n",
       "          'effectual': 1,\n",
       "          'sinner': 1,\n",
       "          'unmanned': 1,\n",
       "          'pavement': 1,\n",
       "          'Ghent': 1,\n",
       "          'Job': 5,\n",
       "          'holies': 1,\n",
       "          'fatally': 2,\n",
       "          'moor': 1,\n",
       "          'whence': 14,\n",
       "          'vest': 4,\n",
       "          'vouchers': 1,\n",
       "          'hovering': 10,\n",
       "          'poisonous': 3,\n",
       "          'summoning': 2,\n",
       "          'orphan': 1,\n",
       "          'limped': 2,\n",
       "          'disks': 1,\n",
       "          'gorges': 2,\n",
       "          'incredible': 9,\n",
       "          'fattening': 1,\n",
       "          'Thus': 25,\n",
       "          'inability': 1,\n",
       "          'vicar': 1,\n",
       "          'BELOW': 1,\n",
       "          'But': 705,\n",
       "          'electric': 1,\n",
       "          'harpooning': 2,\n",
       "          'greenness': 3,\n",
       "          'cassock': 1,\n",
       "          'cloven': 2,\n",
       "          'stumps': 2,\n",
       "          'developments': 1,\n",
       "          'detect': 2,\n",
       "          'uncouthness': 1,\n",
       "          'ATTACKED': 1,\n",
       "          'sanctity': 2,\n",
       "          'abandoned': 7,\n",
       "          'foibles': 1,\n",
       "          'clappings': 1,\n",
       "          'spindled': 1,\n",
       "          'Leap': 1,\n",
       "          'driftings': 1,\n",
       "          'nostrils': 7,\n",
       "          'disappearing': 3,\n",
       "          'Power': 1,\n",
       "          'hain': 1,\n",
       "          'Mate': 1,\n",
       "          '\"...': 1,\n",
       "          'craven': 4,\n",
       "          'vortices': 1,\n",
       "          'Fine': 2,\n",
       "          'cities': 3,\n",
       "          'snow': 30,\n",
       "          'revery': 1,\n",
       "          'partner': 2,\n",
       "          'softest': 1,\n",
       "          'bayonets': 1,\n",
       "          'awaiting': 6,\n",
       "          'blotted': 1,\n",
       "          'fin': 25,\n",
       "          'Lank': 1,\n",
       "          'dolphin': 4,\n",
       "          'minus': 3,\n",
       "          'fantastic': 2,\n",
       "          'hammock': 33,\n",
       "          'foulness': 1,\n",
       "          'liquid': 2,\n",
       "          'incline': 2,\n",
       "          'Hell': 3,\n",
       "          'pious': 9,\n",
       "          'remote': 10,\n",
       "          'remaining': 10,\n",
       "          'bitin': 1,\n",
       "          'kicked': 9,\n",
       "          'missionaries': 1,\n",
       "          'horrible': 14,\n",
       "          'contract': 2,\n",
       "          'match': 9,\n",
       "          'leave': 18,\n",
       "          'DO': 8,\n",
       "          'BALLENA': 1,\n",
       "          'obtain': 3,\n",
       "          'swell': 11,\n",
       "          'unfurnished': 1,\n",
       "          'ing': 1,\n",
       "          'insulated': 2,\n",
       "          'game': 22,\n",
       "          'attar': 1,\n",
       "          ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982),\n",
       " (\"'\", 2684),\n",
       " ('-', 2552),\n",
       " ('his', 2459),\n",
       " ('it', 2209),\n",
       " ('I', 2124),\n",
       " ('s', 1739),\n",
       " ('is', 1695),\n",
       " ('he', 1661),\n",
       " ('with', 1659),\n",
       " ('was', 1632),\n",
       " ('as', 1620),\n",
       " ('\"', 1478),\n",
       " ('all', 1462),\n",
       " ('for', 1414),\n",
       " ('this', 1280),\n",
       " ('!', 1269),\n",
       " ('at', 1231),\n",
       " ('by', 1137),\n",
       " ('but', 1113),\n",
       " ('not', 1103),\n",
       " ('--', 1070),\n",
       " ('him', 1058),\n",
       " ('from', 1052),\n",
       " ('be', 1030),\n",
       " ('on', 1005),\n",
       " ('so', 918),\n",
       " ('whale', 906),\n",
       " ('one', 889),\n",
       " ('you', 841),\n",
       " ('had', 767),\n",
       " ('have', 760),\n",
       " ('there', 715),\n",
       " ('But', 705),\n",
       " ('or', 697),\n",
       " ('were', 680),\n",
       " ('now', 646),\n",
       " ('which', 640),\n",
       " ('?', 637),\n",
       " ('me', 627),\n",
       " ('like', 624)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'N',\n",
       " 'Nr',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__init__',\n",
       " '__ior__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__missing__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_cumulative_frequencies',\n",
       " '_keep_positive',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'elements',\n",
       " 'freq',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hapaxes',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'max',\n",
       " 'most_common',\n",
       " 'pformat',\n",
       " 'plot',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'pprint',\n",
       " 'r_Nr',\n",
       " 'setdefault',\n",
       " 'subtract',\n",
       " 'tabulate',\n",
       " 'unicode_repr',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fdist1) #频率排名前五十"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdist1.plot(50,cumulative=True) #这里会打出图像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cookeri'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('cookery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'antonym'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('antonym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idealist'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('idealistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures',\n",
       " 'BigramCollocationFinder',\n",
       " 'BigramTagger',\n",
       " 'BinaryMaxentFeatureEncoding',\n",
       " 'BlanklineTokenizer',\n",
       " 'BllipParser',\n",
       " 'BottomUpChartParser',\n",
       " 'BottomUpLeftCornerChartParser',\n",
       " 'BottomUpProbabilisticChartParser',\n",
       " 'Boxer',\n",
       " 'BrillTagger',\n",
       " 'BrillTaggerTrainer',\n",
       " 'CFG',\n",
       " 'CRFTagger',\n",
       " 'CfgReadingCommand',\n",
       " 'ChartParser',\n",
       " 'ChunkParserI',\n",
       " 'ChunkScore',\n",
       " 'ClassifierBasedPOSTagger',\n",
       " 'ClassifierBasedTagger',\n",
       " 'ClassifierI',\n",
       " 'ConcordanceIndex',\n",
       " 'ConditionalExponentialClassifier',\n",
       " 'ConditionalFreqDist',\n",
       " 'ConditionalProbDist',\n",
       " 'ConditionalProbDistI',\n",
       " 'ConfusionMatrix',\n",
       " 'ContextIndex',\n",
       " 'ContextTagger',\n",
       " 'ContingencyMeasures',\n",
       " 'CrossValidationProbDist',\n",
       " 'DRS',\n",
       " 'DecisionTreeClassifier',\n",
       " 'DefaultTagger',\n",
       " 'DependencyEvaluator',\n",
       " 'DependencyGrammar',\n",
       " 'DependencyGraph',\n",
       " 'DependencyProduction',\n",
       " 'DictionaryConditionalProbDist',\n",
       " 'DictionaryProbDist',\n",
       " 'DiscourseTester',\n",
       " 'DrtExpression',\n",
       " 'DrtGlueReadingCommand',\n",
       " 'ELEProbDist',\n",
       " 'EarleyChartParser',\n",
       " 'Expression',\n",
       " 'FStructure',\n",
       " 'FeatDict',\n",
       " 'FeatList',\n",
       " 'FeatStruct',\n",
       " 'FeatStructReader',\n",
       " 'Feature',\n",
       " 'FeatureBottomUpChartParser',\n",
       " 'FeatureBottomUpLeftCornerChartParser',\n",
       " 'FeatureChartParser',\n",
       " 'FeatureEarleyChartParser',\n",
       " 'FeatureIncrementalBottomUpChartParser',\n",
       " 'FeatureIncrementalBottomUpLeftCornerChartParser',\n",
       " 'FeatureIncrementalChartParser',\n",
       " 'FeatureIncrementalTopDownChartParser',\n",
       " 'FeatureTopDownChartParser',\n",
       " 'FreqDist',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HeldoutProbDist',\n",
       " 'HiddenMarkovModelTagger',\n",
       " 'HiddenMarkovModelTrainer',\n",
       " 'HunposTagger',\n",
       " 'IBMModel',\n",
       " 'IBMModel1',\n",
       " 'IBMModel2',\n",
       " 'IBMModel3',\n",
       " 'IBMModel4',\n",
       " 'IBMModel5',\n",
       " 'ISRIStemmer',\n",
       " 'ImmutableMultiParentedTree',\n",
       " 'ImmutableParentedTree',\n",
       " 'ImmutableProbabilisticMixIn',\n",
       " 'ImmutableProbabilisticTree',\n",
       " 'ImmutableTree',\n",
       " 'IncrementalBottomUpChartParser',\n",
       " 'IncrementalBottomUpLeftCornerChartParser',\n",
       " 'IncrementalChartParser',\n",
       " 'IncrementalLeftCornerChartParser',\n",
       " 'IncrementalTopDownChartParser',\n",
       " 'Index',\n",
       " 'InsideChartParser',\n",
       " 'JSONTaggedDecoder',\n",
       " 'JSONTaggedEncoder',\n",
       " 'KneserNeyProbDist',\n",
       " 'LancasterStemmer',\n",
       " 'LaplaceProbDist',\n",
       " 'LazyConcatenation',\n",
       " 'LazyEnumerate',\n",
       " 'LazyMap',\n",
       " 'LazySubsequence',\n",
       " 'LazyZip',\n",
       " 'LeftCornerChartParser',\n",
       " 'LidstoneProbDist',\n",
       " 'LineTokenizer',\n",
       " 'LogicalExpressionException',\n",
       " 'LongestChartParser',\n",
       " 'MLEProbDist',\n",
       " 'MWETokenizer',\n",
       " 'Mace',\n",
       " 'MaceCommand',\n",
       " 'MaltParser',\n",
       " 'MaxentClassifier',\n",
       " 'Model',\n",
       " 'MultiClassifierI',\n",
       " 'MultiParentedTree',\n",
       " 'MutableProbDist',\n",
       " 'NaiveBayesClassifier',\n",
       " 'NaiveBayesDependencyScorer',\n",
       " 'NgramAssocMeasures',\n",
       " 'NgramTagger',\n",
       " 'NonprojectiveDependencyParser',\n",
       " 'Nonterminal',\n",
       " 'OrderedDict',\n",
       " 'PCFG',\n",
       " 'Paice',\n",
       " 'ParallelProverBuilder',\n",
       " 'ParallelProverBuilderCommand',\n",
       " 'ParentedTree',\n",
       " 'ParserI',\n",
       " 'PerceptronTagger',\n",
       " 'PhraseTable',\n",
       " 'PorterStemmer',\n",
       " 'PositiveNaiveBayesClassifier',\n",
       " 'ProbDistI',\n",
       " 'ProbabilisticDependencyGrammar',\n",
       " 'ProbabilisticMixIn',\n",
       " 'ProbabilisticNonprojectiveParser',\n",
       " 'ProbabilisticProduction',\n",
       " 'ProbabilisticProjectiveDependencyParser',\n",
       " 'ProbabilisticTree',\n",
       " 'Production',\n",
       " 'ProjectiveDependencyParser',\n",
       " 'Prover9',\n",
       " 'Prover9Command',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'QuadgramCollocationFinder',\n",
       " 'RSLPStemmer',\n",
       " 'RTEFeatureExtractor',\n",
       " 'RandomChartParser',\n",
       " 'RangeFeature',\n",
       " 'ReadingCommand',\n",
       " 'RecursiveDescentParser',\n",
       " 'RegexpChunkParser',\n",
       " 'RegexpParser',\n",
       " 'RegexpStemmer',\n",
       " 'RegexpTagger',\n",
       " 'RegexpTokenizer',\n",
       " 'ResolutionProver',\n",
       " 'ResolutionProverCommand',\n",
       " 'SExprTokenizer',\n",
       " 'SLASH',\n",
       " 'Senna',\n",
       " 'SennaChunkTagger',\n",
       " 'SennaNERTagger',\n",
       " 'SennaTagger',\n",
       " 'SequentialBackoffTagger',\n",
       " 'ShiftReduceParser',\n",
       " 'SimpleGoodTuringProbDist',\n",
       " 'SklearnClassifier',\n",
       " 'SlashFeature',\n",
       " 'SnowballStemmer',\n",
       " 'SpaceTokenizer',\n",
       " 'StackDecoder',\n",
       " 'StanfordNERTagger',\n",
       " 'StanfordPOSTagger',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTagger',\n",
       " 'StanfordTokenizer',\n",
       " 'StemmerI',\n",
       " 'SteppingChartParser',\n",
       " 'SteppingRecursiveDescentParser',\n",
       " 'SteppingShiftReduceParser',\n",
       " 'TYPE',\n",
       " 'TabTokenizer',\n",
       " 'TableauProver',\n",
       " 'TableauProverCommand',\n",
       " 'TaggerI',\n",
       " 'TestGrammar',\n",
       " 'Text',\n",
       " 'TextCat',\n",
       " 'TextCollection',\n",
       " 'TextTilingTokenizer',\n",
       " 'TnT',\n",
       " 'TokenSearcher',\n",
       " 'TopDownChartParser',\n",
       " 'TransitionParser',\n",
       " 'Tree',\n",
       " 'TreebankWordTokenizer',\n",
       " 'Trie',\n",
       " 'TrigramAssocMeasures',\n",
       " 'TrigramCollocationFinder',\n",
       " 'TrigramTagger',\n",
       " 'TweetTokenizer',\n",
       " 'TypedMaxentFeatureEncoding',\n",
       " 'Undefined',\n",
       " 'UniformProbDist',\n",
       " 'UnigramTagger',\n",
       " 'UnsortedChartParser',\n",
       " 'Valuation',\n",
       " 'Variable',\n",
       " 'ViterbiParser',\n",
       " 'WekaClassifier',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WittenBellProbDist',\n",
       " 'WordNetLemmatizer',\n",
       " 'WordPunctTokenizer',\n",
       " '__author__',\n",
       " '__author_email__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__classifiers__',\n",
       " '__copyright__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__keywords__',\n",
       " '__license__',\n",
       " '__loader__',\n",
       " '__longdescr__',\n",
       " '__maintainer__',\n",
       " '__maintainer_email__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__url__',\n",
       " '__version__',\n",
       " 'absolute_import',\n",
       " 'accuracy',\n",
       " 'add_logs',\n",
       " 'agreement',\n",
       " 'alignment_error_rate',\n",
       " 'api',\n",
       " 'app',\n",
       " 'apply_features',\n",
       " 'approxrand',\n",
       " 'arity',\n",
       " 'association',\n",
       " 'bigrams',\n",
       " 'binary_distance',\n",
       " 'binary_search_file',\n",
       " 'binding_ops',\n",
       " 'bisect',\n",
       " 'blankline_tokenize',\n",
       " 'bleu',\n",
       " 'bleu_score',\n",
       " 'bllip',\n",
       " 'book',\n",
       " 'boolean_ops',\n",
       " 'boxer',\n",
       " 'bracket_parse',\n",
       " 'breadth_first',\n",
       " 'brill',\n",
       " 'brill_trainer',\n",
       " 'build_opener',\n",
       " 'call_megam',\n",
       " 'casual',\n",
       " 'casual_tokenize',\n",
       " 'ccg',\n",
       " 'chain',\n",
       " 'chart',\n",
       " 'chat',\n",
       " 'choose',\n",
       " 'chunk',\n",
       " 'class_types',\n",
       " 'classify',\n",
       " 'clause',\n",
       " 'clean_html',\n",
       " 'clean_url',\n",
       " 'cluster',\n",
       " 'collocations',\n",
       " 'combinations',\n",
       " 'compat',\n",
       " 'config_java',\n",
       " 'config_megam',\n",
       " 'config_weka',\n",
       " 'conflicts',\n",
       " 'confusionmatrix',\n",
       " 'conllstr2tree',\n",
       " 'conlltags2tree',\n",
       " 'corpus',\n",
       " 'crf',\n",
       " 'custom_distance',\n",
       " 'data',\n",
       " 'decisiontree',\n",
       " 'decorator',\n",
       " 'decorators',\n",
       " 'defaultdict',\n",
       " 'demo',\n",
       " 'dependencygraph',\n",
       " 'deque',\n",
       " 'discourse',\n",
       " 'distance',\n",
       " 'download',\n",
       " 'download_gui',\n",
       " 'download_shell',\n",
       " 'downloader',\n",
       " 'draw',\n",
       " 'drt',\n",
       " 'earleychart',\n",
       " 'edit_distance',\n",
       " 'elementtree_indent',\n",
       " 'entropy',\n",
       " 'equality_preds',\n",
       " 'evaluate',\n",
       " 'evaluate_sents',\n",
       " 'everygrams',\n",
       " 'extract_rels',\n",
       " 'extract_test_sentences',\n",
       " 'f_measure',\n",
       " 'featstruct',\n",
       " 'featurechart',\n",
       " 'filestring',\n",
       " 'flatten',\n",
       " 'fractional_presence',\n",
       " 'getproxies',\n",
       " 'ghd',\n",
       " 'glue',\n",
       " 'grammar',\n",
       " 'guess_encoding',\n",
       " 'help',\n",
       " 'hmm',\n",
       " 'hunpos',\n",
       " 'ibm1',\n",
       " 'ibm2',\n",
       " 'ibm3',\n",
       " 'ibm4',\n",
       " 'ibm5',\n",
       " 'ibm_model',\n",
       " 'ieerstr2tree',\n",
       " 'in_idle',\n",
       " 'induce_pcfg',\n",
       " 'inference',\n",
       " 'infile',\n",
       " 'install_opener',\n",
       " 'internals',\n",
       " 'interpret_sents',\n",
       " 'interval_distance',\n",
       " 'invert_dict',\n",
       " 'invert_graph',\n",
       " 'is_rel',\n",
       " 'islice',\n",
       " 'isri',\n",
       " 'jaccard_distance',\n",
       " 'json_tags',\n",
       " 'jsontags',\n",
       " 'lancaster',\n",
       " 'lazyimport',\n",
       " 'lfg',\n",
       " 'line_tokenize',\n",
       " 'linearlogic',\n",
       " 'load',\n",
       " 'load_parser',\n",
       " 'locale',\n",
       " 'log_likelihood',\n",
       " 'logic',\n",
       " 'mace',\n",
       " 'malt',\n",
       " 'map_tag',\n",
       " 'mapping',\n",
       " 'masi_distance',\n",
       " 'maxent',\n",
       " 'megam',\n",
       " 'memoize',\n",
       " 'metrics',\n",
       " 'misc',\n",
       " 'mwe',\n",
       " 'naivebayes',\n",
       " 'ne_chunk',\n",
       " 'ne_chunk_sents',\n",
       " 'ngrams',\n",
       " 'nonprojectivedependencyparser',\n",
       " 'nonterminals',\n",
       " 'numpy',\n",
       " 'os',\n",
       " 'pad_sequence',\n",
       " 'paice',\n",
       " 'parse',\n",
       " 'parse_sents',\n",
       " 'pchart',\n",
       " 'perceptron',\n",
       " 'pk',\n",
       " 'porter',\n",
       " 'pos_tag',\n",
       " 'pos_tag_sents',\n",
       " 'positivenaivebayes',\n",
       " 'pprint',\n",
       " 'pr',\n",
       " 'precision',\n",
       " 'presence',\n",
       " 'print_function',\n",
       " 'print_string',\n",
       " 'probability',\n",
       " 'projectivedependencyparser',\n",
       " 'prover9',\n",
       " 'punkt',\n",
       " 'py25',\n",
       " 'py26',\n",
       " 'py27',\n",
       " 'pydoc',\n",
       " 'python_2_unicode_compatible',\n",
       " 'raise_unorderable_types',\n",
       " 'ranks_from_scores',\n",
       " 'ranks_from_sequence',\n",
       " 're',\n",
       " 're_show',\n",
       " 'read_grammar',\n",
       " 'read_logic',\n",
       " 'read_valuation',\n",
       " 'recall',\n",
       " 'recursivedescent',\n",
       " 'regexp',\n",
       " 'regexp_span_tokenize',\n",
       " 'regexp_tokenize',\n",
       " 'register_tag',\n",
       " 'relextract',\n",
       " 'resolution',\n",
       " 'ribes',\n",
       " 'ribes_score',\n",
       " 'root_semrep',\n",
       " 'rslp',\n",
       " 'rte_classifier',\n",
       " 'rte_classify',\n",
       " 'rte_features',\n",
       " 'rtuple',\n",
       " 'scikitlearn',\n",
       " 'scores',\n",
       " 'segmentation',\n",
       " 'sem',\n",
       " 'senna',\n",
       " 'sent_tokenize',\n",
       " 'sequential',\n",
       " 'set2rel',\n",
       " 'set_proxy',\n",
       " 'sexpr',\n",
       " 'sexpr_tokenize',\n",
       " 'shiftreduce',\n",
       " 'simple',\n",
       " 'sinica_parse',\n",
       " 'six',\n",
       " 'skipgrams',\n",
       " 'skolemize',\n",
       " 'slice_bounds',\n",
       " 'snowball',\n",
       " 'spearman',\n",
       " 'spearman_correlation',\n",
       " 'stack_decoder',\n",
       " 'stanford',\n",
       " 'stanford_segmenter',\n",
       " 'stem',\n",
       " 'str2tuple',\n",
       " 'string_span_tokenize',\n",
       " 'string_types',\n",
       " 'subprocess',\n",
       " 'subsumes',\n",
       " 'sum_logs',\n",
       " 'tableau',\n",
       " 'tadm',\n",
       " 'tag',\n",
       " 'tagset_mapping',\n",
       " 'tagstr2tree',\n",
       " 'tbl',\n",
       " 'text',\n",
       " 'text_type',\n",
       " 'textcat',\n",
       " 'texttiling',\n",
       " 'textwrap',\n",
       " 'tkinter',\n",
       " 'tnt',\n",
       " 'tokenize',\n",
       " 'tokenwrap',\n",
       " 'toolbox',\n",
       " 'total_ordering',\n",
       " 'transitionparser',\n",
       " 'transitive_closure',\n",
       " 'translate',\n",
       " 'tree',\n",
       " 'tree2conllstr',\n",
       " 'tree2conlltags',\n",
       " 'treebank',\n",
       " 'treetransforms',\n",
       " 'trigrams',\n",
       " 'tuple2str',\n",
       " 'types',\n",
       " 'unify',\n",
       " 'unique_list',\n",
       " 'untag',\n",
       " 'usage',\n",
       " 'util',\n",
       " 'version_file',\n",
       " 'version_info',\n",
       " 'viterbi',\n",
       " 'weka',\n",
       " 'windowdiff',\n",
       " 'word_tokenize',\n",
       " 'wordnet',\n",
       " 'wordpunct_tokenize',\n",
       " 'wsd']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer=nltk.PorterStemmer()\n",
    "verbs=['ran','run','running','walk','walker','ride']\n",
    "stems=[]\n",
    "for verb in verbs:\n",
    "    stemmed_verb=stemmer.stem(verb)\n",
    "    stems.append(stemmed_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ran', 'ride', 'run', 'walk', 'walker']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer #两种导入方法都可以提取单个、多个词的词干\n",
    "stemmer=PorterStemmer()\n",
    "verbs=['ran','run','running','walk','walker','ride']\n",
    "stems=[]\n",
    "for verb in verbs:\n",
    "    stemmed_verb=stemmer.stem(verb)\n",
    "    stems.append(stemmed_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ran', 'ride', 'run', 'walk', 'walker']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单词变位原型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cooking'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('cooking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cooking',pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cook'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('cooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RegexpReplacer' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f50f25dbec0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mreplacers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpReplacer\u001b[0m  \u001b[1;31m#为什么！！！\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mreplacer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpReplacer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mreplacer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"You can't do this, can you?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RegexpReplacer' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from replacers import RegexpReplacer  #为什么！！！\n",
    "replacer = RegexpReplacer()\n",
    "replacer.replace(\"You can't do this, can you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### accessing corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt 4.609909212324673 24.822884416924666 24.63538599411087\n",
      "austen-persuasion.txt 4.749793727271801 26.19989324793168 16.00962165688193\n",
      "austen-sense.txt 4.753785952421314 28.32086417283457 20.719449729255086\n",
      "bible-kjv.txt 4.286881563819072 33.57319868451649 73.40068269300603\n",
      "blake-poems.txt 4.567033756284415 19.073059360730593 4.59010989010989\n",
      "bryant-stories.txt 4.489300433741879 19.40726510653161 12.57081447963801\n",
      "burgess-busterbrown.txt 4.464641670621737 17.99146110056926 10.75\n",
      "carroll-alice.txt 4.233216065669891 20.029359953024077 11.309681697612731\n",
      "chesterton-ball.txt 4.716173862839705 20.296296296296298 10.841175813121717\n",
      "chesterton-brown.txt 4.724783007796614 22.61245401996847 10.37028557657549\n",
      "chesterton-thursday.txt 4.63099417739442 18.496258685195084 10.167915381225209\n",
      "edgeworth-parents.txt 4.4391184023772565 20.59266862170088 21.960075054727405\n",
      "melville-moby_dick.txt 4.76571875515204 25.928919375683467 13.502044830977896\n",
      "milton-paradise.txt 4.835734572682675 52.309562398703406 9.00613896381732\n",
      "shakespeare-caesar.txt 4.347539968257655 11.943134535367545 7.256460674157303\n",
      "shakespeare-hamlet.txt 4.3597698072805136 12.028332260141662 6.858821369561227\n",
      "shakespeare-macbeth.txt 4.336689714779602 12.134242265338228 5.760517799352751\n",
      "whitman-leaves.txt 4.591950052620365 36.44305882352941 10.809058552585665\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "for filename in gutenberg.fileids():\n",
    "    r=gutenberg.raw(filename)\n",
    "    w=gutenberg.words(filename)\n",
    "    s=gutenberg.sents(filename)\n",
    "    v=set(w)\n",
    "    print (filename,len(r)/len(w),len(w)/len(s),len(w)/len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 11 of 11 matches:\n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r\n",
      "ll over with a heathenish array of monstrous clubs and spears . Some were thick\n",
      "d as you gazed , and wondered what monstrous cannibal and savage could ever hav\n",
      "that has survived the flood ; most monstrous and most mountainous ! That Himmal\n",
      "they might scout at Moby Dick as a monstrous fable , or still worse and more de\n",
      "th of Radney .'\" CHAPTER 55 Of the Monstrous Pictures of Whales . I shall ere l\n",
      "ing Scenes . In connexion with the monstrous pictures of whales , I am strongly\n",
      "ere to enter upon those still more monstrous stories of them which are to be fo\n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u\n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 外部文档操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open('E:/大三下学期/高级编程（一）/第一周 20180306/corpara/1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The next financial crisis may be triggered by central banks.\\n\\nAs with London buses, don鈥檛 worry if you miss a financial crisis; another will be along shortly.\\n\\nThe latest study on long-term asset returns from Deutsche Bank shows that crises in developed markets have become much more common in recent decades.\\n\\nThat does not bode well.\\n\\nDeutsche defines a crisis as a period when a country suffers one of the following: a 15% annual decline in equities; a 10% fall in its currency or its government bonds; a default on its national debt; or a period of double-digit inflation.\\n\\nDuring the 19th century, only occasionally did more than half of countries for which there are data suffer such a shock in a single year.\\n\\nBut since the 1980s, in numerous years more than half of them have been in a financial crisis of some kind.\\n\\nThe main reason for this, argues Deutsche, is the monetary system.\\n\\nUnder the gold standard and its successor, the Bretton Woods system of fixed exchange rates, the amount of credit creation was limited.\\n\\nA country that expanded its money supply too quickly would suffer a trade deficit and pressure on its currency鈥檚 exchange rate; the government would react by slamming on the monetary brakes.\\n\\nThe result was that it was harder for financial bubbles to inflate.\\n\\nBut since the early 1970s more countries have moved to a floating exchange-rate system.\\n\\nThis gives governments the flexibility to deal with an economic crisis, and means they do not have to subordinate other policy goals to maintaining a currency peg.\\n\\nIt has also created a trend towards greater trade imbalances, which no longer constrain policymakers鈥攖he currency is often allowed to take the strain.\\n\\nSimilarly, government debt has risen steadily as a proportion of GDP since the mid-1970s.\\n\\nThere has been little pressure from the markets to balance the budget; Japan has had a deficit every year since 1966, and France since 1993.\\n\\nItaly has managed just one year of surplus since 1950.\\n\\nIn the developed world, consumers and companies have also taken on more debt.\\n\\nThe result has been a cycle of credit expansion and collapse.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', '2.txt', '3.txt', '4.txt', '5.txt']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#建立自己的语料库\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = \"E:/大三下学期/高级编程（一）/第一周 20180306/corpara\"\n",
    "wordlists=PlaintextCorpusReader(corpus_root,'.*\\.txt')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "alf of countries for which there are data suffer such a shock in a single year\n"
     ]
    }
   ],
   "source": [
    "#进行检索\n",
    "n=nltk.word_tokenize(wordlists.raw(fileids=\"1.txt\"))\n",
    "complete1=nltk.Text(n)\n",
    "complete1.concordance(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"zh-cn\">\\r\\n<head>\\r\\n<meta charse'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取网络文本\n",
    "from urllib.request import urlopen\n",
    "url=\"https://www.cnblogs.com/ArrozZhu/p/8463882.html\"\n",
    "html=urlopen(url).read()\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"zh-cn\">\\r\\n<head>\\r\\n<meta charset=\"utf-8\"/>\\r\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\\r\\n<title>&lt;Python Text Processing with NLTK 2.0 Cookbook&gt;\\xe4\\xbb\\xa3\\xe7\\xa0\\x81\\xe7\\xac\\x94\\xe8\\xae\\xb0 - Arroz - \\xe5\\x8d\\x9a\\xe5\\xae\\xa2\\xe5\\x9b\\xad</title>\\r\\n<link type=\"text/css\" rel=\"stylesheet\" href=\"/bundles/blog-common.css?v=-hy83QNg62d4qYibixJzxMJkbf1P9fTBlqv7SK5zVL01\"/>\\n<link id=\"MainCss\" type=\"text/css\" rel=\"stylesheet\" href=\"/skins/gray/bundle-gray.css?v=HL2WWD0LJEIKd_qaZrrRKuZpscDuS7PCV4RyAclJuYE1\"/>\\n<link id=\"mobile-style\" media=\"only screen and (max-width: 767px)\" type=\"text/css\" rel=\"stylesheet\" href=\"/skins/gray/bundle-gray-mobile.css?v=Owzi85UhDEb1CZicvxyFsKox3GEEaJ6PdwmXJEqO7dc1\"/>\\r\\n<link title=\"RSS\" type=\"application/rss+xml\" rel=\"alternate\" href=\"http://www.cnblogs.com/ArrozZhu/rss\"/>\\r\\n<link title=\"RSD\" type=\"application/rsd+xml\" rel=\"EditURI\" href=\"http://www.cnblogs.com/ArrozZhu/rsd.xml\"/>\\n<link type=\"application/wlwmanifest+xml\" rel=\"wlwmanifest\" href=\"http://www.cnblogs.com/ArrozZhu/wlwmanifest.xml\"/>\\r\\n<script src=\"//common.cnblogs.com/scripts/jquery-2.2.0.min.js\"></script>\\r\\n<script type=\"text/javascript\">var currentBlogApp = \\'ArrozZhu\\', cb_enable_mathjax=false;var isLogined=false;</script>\\r\\n<script src=\"/bundles/blog-common.js?v=taItysi72HxMPeH9Xg5nAYabRul6hhgahi3tVIMIKV81\" type=\"text/javascript\"></script>\\r\\n</head>\\r\\n<body>\\r\\n<a name=\"top\"></a>\\r\\n\\r\\n<!--done-->\\r\\n<div id=\"banner\"><div id=\"bnr_pic\">\\r\\n<!--done-->\\r\\n<div class=\"header\">\\r\\n\\t<div class=\"headerText\">\\r\\n\\t\\t<a id=\"Header1_HeaderTitle\" class=\"headermaintitle\" href=\"http://www.cnblogs.com/ArrozZhu/\">Arroz</a><br>\\r\\n\\t</div>\\r\\n\\t<div class=\"headerDis\"></div>\\r\\n</div>\\r\\n</div></div>\\r\\n<div id=\"main\">\\r\\n\\t<!-- left starts -->\\r\\n\\t<div id=\"left\">\\r\\n\\t<div id=\"left_border\">\\r\\n\\t\\t<DIV id=\"mystats\">\\r\\n\\t\\t\\t<br>\\r\\n\\t\\t\\t\\t<div id=\"blog_stats\">\\r\\n<!--done-->\\r\\n<div class=\"blogStats\">\\r\\n\\xe9\\x9a\\x8f\\xe7\\xac\\x94- 109&nbsp;\\r\\n\\xe6\\x96\\x87\\xe7\\xab\\xa0- 24&nbsp;\\r\\n\\xe8\\xaf\\x84\\xe8\\xae\\xba- 0&nbsp;\\r\\n\\r\\n</div></div>\\r\\n\\t\\t\\t\\r\\n\\t\\t</DIV>\\r\\n\\t\\t<div id=\"mylinks\">\\r\\n<!--done-->\\r\\n<a id=\"blog_nav_sitehome\" class=\"menu\" href=\"http://www.cnblogs.com/\">\\xe5\\x8d\\x9a\\xe5\\xae\\xa2\\xe5\\x9b\\xad</a>&nbsp;&nbsp;<a id=\"blog_nav_myhome\" class=\"menu\" href=\"http://www.cnblogs.com/ArrozZhu/\">\\xe9\\xa6\\x96\\xe9\\xa1\\xb5</a>&nbsp;&nbsp;<a id=\"blog_nav_newpost\" class=\"menu\" rel=\"nofollow\" href=\"https://i.cnblogs.com/EditPosts.aspx?opt=1\">\\xe6\\x96\\xb0\\xe9\\x9a\\x8f\\xe7\\xac\\x94</a>&nbsp;&nbsp;<a id=\"MyLinks1_NewArticleLink\" class=\"menu\" href=\"../../EnterMyBlog.aspx?NewArticle=1\">\\xe6\\x96\\xb0\\xe6\\x96\\x87\\xe7\\xab\\xa0</a>&nbsp;&nbsp;<a id=\"blog_nav_contact\" accesskey=\"9\" class=\"menu\" rel=\"nofollow\" href=\"https://msg.cnblogs.com/send/Arroz\">\\xe8\\x81\\x94\\xe7\\xb3\\xbb</a>&nbsp;&nbsp;<a id=\"blog_nav_admin\" class=\"menu\" rel=\"nofollow\" href=\"https://i.cnblogs.com/\">\\xe7\\xae\\xa1\\xe7\\x90\\x86</a>&nbsp;&nbsp;<a id=\"blog_nav_rss\" class=\"menu\" href=\"http://www.cnblogs.com/ArrozZhu/rss\">\\xe8\\xae\\xa2\\xe9\\x98\\x85</a>&nbsp;<a id=\"blog_nav_rss_image\" href=\"http://www.cnblogs.com/ArrozZhu/rss\"><img src=\"//www.cnblogs.com/images/xml.gif\" alt=\"\\xe8\\xae\\xa2\\xe9\\x98\\x85\" /></a></div>\\r\\n\\t\\t<div id=\"topics\">\\r\\n\\t\\t\\t\\r\\n<div id=\"post_detail\">\\r\\n<!--done-->\\r\\n<div class = \"post\">\\r\\n\\t<div class = \"postTitle\">\\r\\n\\t\\t<a id=\"cb_post_title_url\" class=\"postTitle2\" href=\"http://www.cnblogs.com/ArrozZhu/p/8463882.html\">&lt;Python Text Processing with NLTK 2.0 Cookbook&gt;\\xe4\\xbb\\xa3\\xe7\\xa0\\x81\\xe7\\xac\\x94\\xe8\\xae\\xb0</a>\\r\\n\\t</div>\\r\\n\\t<div id=\"cnblogs_post_body\" class=\"blogpost-body\"><p><span style=\"font-size: 12pt;\">\\xe5\\xa6\\x82\\xe4\\xb8\\x8b\\xe6\\x98\\xaf&lt;Python Text Processing with NLTK 2.0 Cookbook&gt;\\xe4\\xb8\\x80\\xe4\\xb9\\xa6\\xe9\\x83\\xa8\\xe5\\x88\\x86\\xe7\\xab\\xa0\\xe8\\x8a\\x82\\xe7\\x9a\\x84\\xe4\\xbb\\xa3\\xe7\\xa0\\x81\\xe7\\xac\\x94\\xe8\\xae\\xb0. </span></p>\\r\\n<div style=\"margin-left: 4pt;\">\\r\\n<table style=\"border-collapse: collapse;\" border=\"0\"><colgroup><col style=\"width: 721px;\" /></colgroup>\\r\\n<tbody valign=\"top\">\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Tokenizing text into sentences</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; para = <span style=\"color: #00aa00;\"><em>\"Hello World. It\\'s good to see you. Thanks for buying this book.\"</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> sent_tokenize</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; sent_tokenize(para) <span style=\"color: red;\"># \"sent_tokenize\"</span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe5\\x87\\xbd\\xe6\\x95\\xb0\\xef\\xbc\\x8c\\xe4\\xb8\\x8b\\xe6\\x96\\x87\\xe5\\xbe\\x88\\xe5\\xa4\\x9a\\xe4\\xb8\\xad\\xe9\\x97\\xb4\\xe5\\xb8\\xa6\\xe4\\xb8\\x8b\\xe5\\x88\\x92\\xe7\\xba\\xbf\\xe7\\x9a\\x84\\xe6\\xa0\\x87\\xe8\\xaf\\x86\\xe7\\xac\\xa6\\xe9\\x83\\xbd\\xe6\\x8c\\x87\\xe7\\x9a\\x84\\xe6\\x98\\xaf\\xe5\\x87\\xbd\\xe6\\x95\\xb0\\xe3\\x80\\x82</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Hello World.\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"It\\'s good to see you.\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'Thanks for buying this</em></span> </span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">book.\\'] </span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Tokenizing sentences into words</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> word_tokenize</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; word_tokenize(<span style=\"color: #00aa00;\"><em>\\'Hello World.\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Hello\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'World\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xad\\x89\\xe5\\x90\\x8c\\xe4\\xba\\x8e</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> TreebankWordTokenizer</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tokenizer = TreebankWordTokenizer()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; tokenizer.tokenize(<span style=\"color: #00aa00;\"><em>\\'Hello World.\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Hello\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'World\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">] </span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xad\\x89\\xe5\\x90\\x8c\\xe4\\xba\\x8e</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> nltk</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; text = <span style=\"color: #00aa00;\"><em>\"Hello. Isn\\'t this fun?\"</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; pattern = r<span style=\"color: #00aa00;\"><em>\"\\\\w+|[^\\\\w\\\\s]+\"</em><span style=\"color: black;\"> <span style=\"color: red;\"># r</span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9a</span><span style=\"color: red;\"><span style=\"font-family: Consolas;\">regular expression</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9b\\xe5\\x8f\\x8c\\xe5\\xbc\\x95\\xe5\\x8f\\xb7</span><span style=\"font-family: Consolas;\">\"\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe7\\x94\\xa8\\xe5\\x8d\\x95\\xe5\\xbc\\x95\\xe5\\x8f\\xb7</span><span style=\"font-family: Consolas;\">\\'\\'</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xbb\\xa3\\xe6\\x9b\\xbf\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\">\\\\w</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\xe5\\x8d\\x95\\xe8\\xaf\\x8d\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xef\\xbc\\x8c\\xe7\\xad\\x89\\xe5\\x90\\x8c\\xe4\\xba\\x8e\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe9\\x9b\\x86\\xe5\\x90\\x88</span><span style=\"font-family: Consolas;\">[a-zA-Z0-9_]</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\">+</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\xe4\\xb8\\x80\\xe6\\xac\\xa1\\xe6\\x88\\x96\\xe8\\x80\\x85\\xe5\\xa4\\x9a\\xe6\\xac\\xa1\\xef\\xbc\\x8c\\xe7\\xad\\x89\\xe5\\x90\\x8c\\xe4\\xba\\x8e</span><span style=\"font-family: Consolas;\">{1,}</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x8c\\xe5\\x8d\\xb3</span><span style=\"font-family: Consolas;\">c+ </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x92\\x8c</span><span style=\"font-family: Consolas;\"> c{1,} </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe6\\x84\\x8f\\xe6\\x80\\x9d\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\">\"|\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9a\\xe4\\xba\\x8c\\xe9\\x80\\x89\\xe4\\xb8\\x80\\xef\\xbc\\x8c\\xe6\\xad\\xa3\\xe5\\x88\\x99\\xe8\\xa1\\xa8\\xe8\\xbe\\xbe\\xe5\\xbc\\x8f\\xe4\\xb8\\xad\\xe7\\x9a\\x84</span><span style=\"font-family: Consolas;\">\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x88\\x96</span><span style=\"font-family: Consolas;\">\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\"> [...]</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9a\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe9\\x9b\\x86\\xef\\xbc\\x88\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe7\\xb1\\xbb\\xef\\xbc\\x89\\xef\\xbc\\x8c\\xe5\\x85\\xb6\\xe5\\xaf\\xb9\\xe5\\xba\\x94\\xe7\\x9a\\x84\\xe4\\xbd\\x8d\\xe7\\xbd\\xae\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe6\\x98\\xaf\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe9\\x9b\\x86\\xe4\\xb8\\xad\\xe4\\xbb\\xbb\\xe6\\x84\\x8f\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xef\\xbc\\x8c\\xe4\\xbe\\x8b\\xe5\\xa6\\x82\\xef\\xbc\\x8c</span><span style=\"font-family: Consolas;\">a[bcd]</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xa1\\xa8</span><span style=\"font-family: Consolas;\">abe</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe3\\x80\\x81</span><span style=\"font-family: Consolas;\">ace</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x92\\x8c</span><span style=\"font-family: Consolas;\">ade</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\">^</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\xe5\\x8f\\xaa\\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe4\\xb8\\xb2\\xe7\\x9a\\x84\\xe5\\xbc\\x80\\xe5\\xa4\\xb4\\xef\\xbc\\x9b</span><span style=\"font-family: Consolas;\">\\\\s</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe5\\x8d\\x95\\xe4\\xb8\\xaa\\xe7\\xa9\\xba\\xe6\\xa0\\xbc\\xef\\xbc\\x8c\\xe7\\xad\\x89\\xe5\\x90\\x8c\\xe4\\xba\\x8e</span><span style=\"font-family: Consolas;\">[\\\\f\\\\n\\\\r\\\\t\\\\v]</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> nltk.tokenize.regexp_tokenize(text, pattern)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Hello\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'Isn\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"\\'\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'t\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'this\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'fun\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'?\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Tokenizing sentences using regular expressions</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> RegexpTokenizer</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; tokenizer = RegexpTokenizer(<span style=\"color: #00aa00;\"><em>\"[\\\\w\\']+\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; tokenizer.tokenize(<span style=\"color: #00aa00;\"><em>\"Can\\'t is a contraction.\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\"Can\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'a\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'contraction\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># a simple helper function</span><span style=\"color: silver; font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe3\\x80\\x82</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">the <span style=\"color: blue;\">class<span style=\"color: black;\">.</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> regexp_tokenize</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; regexp_tokenize(<span style=\"color: #00aa00;\"><em>\"Can\\'t is a contraction.\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"[\\\\w\\']+\"</em><span style=\"color: black;\">)</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\"Can\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'a\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'contraction\\'</em><span style=\"color: black;\">] </span></span></span></span></span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Training a sentence tokenizer</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> PunktSentenceTokenizer</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> webtext</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; text = webtext.raw(<span style=\"color: #00aa00;\"><em>\\'overheard.txt\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; sent_tokenizer = PunktSentenceTokenizer(text)</span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-family: Consolas; font-size: 12pt;\"># Let\\'s compare the results to the default sentence tokenizer, as follows: </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; sents1 = sent_tokenizer.tokenize(text)</span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; sents1[<span style=\"color: maroon;\">0<span style=\"color: black;\">] #</span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xaf\\xb7\\xe6\\xb3\\xa8\\xe6\\x84\\x8f\\xef\\xbc\\x8c\\xe7\\xb4\\xa2\\xe5\\xbc\\x95\\xe4\\xbb\\x8e\\xe9\\x9b\\xb6\\xe5\\xbc\\x80\\xe5\\xa7\\x8b\\xef\\xbc\\x9a\\xe7\\xac\\xac</span><span style=\"font-family: Consolas;\">0 </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\xaa\\xe5\\x85\\x83\\xe7\\xb4\\xa0\\xe5\\x86\\x99\\xe4\\xbd\\x9c</span><span style=\"font-family: Consolas;\">sent[0]</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x8c\\xe5\\x85\\xb6\\xe5\\xae\\x9e\\xe6\\x98\\xaf\\xe7\\xac\\xac</span><span style=\"font-family: Consolas;\">1 </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\xaa\\xe8\\xaf\\x8d\"</span><span style=\"font-family: Consolas;\">word1</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"\\xef\\xbc\\x9b\\xe8\\x80\\x8c\\xe5\\x8f\\xa5\\xe5\\xad\\x90\\xe7\\x9a\\x84\\xe7\\xac\\xac</span><span style=\"font-family: Consolas;\">9 </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\xaa\\xe5\\x85\\x83\\xe7\\xb4\\xa0\\xe6\\x98\\xaf\"</span><span style=\"font-family: Consolas;\">word10</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"\\xe3\\x80\\x82</span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'White guy: So, do you have any plans for this evening?\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> sent_tokenize</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; sents2 = sent_tokenize(text)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; sents2[<span style=\"color: maroon;\">0<span style=\"color: black;\">]</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'White guy: So, do you have any plans for this evening?\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; sents1[<span style=\"color: maroon;\">678<span style=\"color: black;\">]</span> </span></span></p>\\r\\n<p><span style=\"color: #00aa00; font-family: Consolas; font-size: 12pt;\"><em>\\'Girl: But <span style=\"color: black;\">you<span style=\"color: #00aa00;\"> already have a Big Mac...\\'</span></span></em> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; sents2[<span style=\"color: maroon;\">678<span style=\"color: black;\">]</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'Girl: But you already have a Big Mac...\\\\\\\\nHobo: Oh, this is all</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">theatrical.<span style=\"color: #00aa00;\"><em>\\' </em></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Filtering stopwords in a tokenized sentence</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> stopwords</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; english_stops = set(stopwords.words(<span style=\"color: #00aa00;\"><em>\\'english\\'</em><span style=\"color: black;\">))</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; words = [<span style=\"color: #00aa00;\"><em>\"Can\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'a\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'contraction\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; [word <span style=\"color: blue;\">for<span style=\"color: black;\"> word <span style=\"color: blue;\">in<span style=\"color: black;\"> words <span style=\"color: blue;\">if<span style=\"color: black;\"> word <span style=\"color: blue;\">not<span style=\"color: black;\"> <span style=\"color: blue;\">in<span style=\"color: black;\"> english_stops]</span> </span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\"Can\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'contraction\\'</em><span style=\"color: black;\">] </span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Looking up synsets for a word in WordNet</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xe4\\xb8\\x80</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; syn = wordnet.synsets(<span style=\"color: #00aa00;\"><em>\\'cookbook\\'</em><span style=\"color: black;\">)<span style=\"background-color: yellow;\">[<span style=\"color: maroon;\">0<span style=\"color: black;\">]</span></span></span> </span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; syn.name()</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'cookbook.n.01\\'</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; syn.definition()</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'a book of recipes and cooking directions\\'</em></span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xe4\\xba\\x8c</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; syn = wordnet.synsets(<span style=\"color: #00aa00;\"><em>\\'motorcar\\'</em><span style=\"color: black;\">)[<span style=\"color: maroon;\">0<span style=\"color: black;\">]</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; syn.name</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&lt;bound method Synset.name of Synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">)&gt;</span> </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet <span style=\"color: blue;\"><span style=\"background-color: yellow;\">as<span style=\"color: black;\"> wn</span></span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; wn.synsets(<span style=\"color: #00aa00;\"><em>\"motorcar\"</em><span style=\"color: black;\">) # </span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x8b\\xac\\xe5\\x8f\\xb7\\xe5\\x86\\x85\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe6\\x98\\xaf\\xe5\\x8d\\x95\\xe5\\xbc\\x95\\xe5\\x8f\\xb7</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[Synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">)] </span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># WordNet</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\"><span style=\"background-color: yellow;\">\\xe5\\xb1\\x82\\xe6\\xac\\xa1\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xef\\xbc\\x88\\xe8\\xaf\\x8d\\xe6\\xb1\\x87\\xe5\\x85\\xb3\\xe7\\xb3\\xbb\\xef\\xbc\\x89</span><span style=\"color: black;\">&mdash;&mdash;</span></span><span style=\"font-family: Consolas;\">Hypernym /\\xcb\\x88ha<span style=\"color: black;\">\\xc9\\xaap\\xc9\\x99n\\xc9\\xaam//hyponym/ \\xcb\\x88ha\\xc9\\xaap\\xc9\\x99n\\xc9\\xaam / relation&mdash;&mdash;</span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x8a\\xe7\\xba\\xa7\\xe6\\xa6\\x82\\xe5\\xbf\\xb5\\xe4\\xb8\\x8e\\xe4\\xbb\\x8e\\xe5\\xb1\\x9e\\xe6\\xa6\\x82\\xe5\\xbf\\xb5\\xe7\\x9a\\x84\\xe5\\x85\\xb3\\xe7\\xb3\\xbb</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet <span style=\"color: blue;\"><span style=\"background-color: yellow;\">as<span style=\"color: black;\"> wn</span></span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; motorcar = wn.synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; types_of_motorcar = motorcar.hyponyms() #</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xad\\x89\\xe5\\x8f\\xb7\\xe4\\xb8\\xa4\\xe8\\xbe\\xb9\\xe7\\x9a\\x84\\xe8\\xa1\\xa8\\xe8\\xbe\\xbe\\xe5\\xbc\\x8f\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe6\\x8d\\xa2\\xe4\\xbd\\x8d\\xef\\xbc\\x8c\\xe5\\x90\\xa6\\xe5\\x88\\x99\\xe4\\xbc\\x9a\\xe5\\x87\\xba\\xe7\\x8e\\xb0\\xe8\\xad\\xa6\\xe7\\xa4\\xba\\xef\\xbc\\x9a</span><span style=\"font-family: Consolas;\">can\\'t assign to function call. </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; types_of_motorcar</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[Synset(<span style=\"color: #00aa00;\"><em>\\'ambulance.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'beach_wagon.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'bus.n.04\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'cab.n.03\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'compact.n.03\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'convertible.n.01\\'</em><span style=\"color: black;\">) ... Synset(<span style=\"color: #00aa00;\"><em>\\'stanley_steamer.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'stock_car.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'subcompact.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'touring_car.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'used-car.n.01\\'</em><span style=\"color: black;\">)] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xe9\\x83\\xa8\\xe5\\x88\\x86\\xe6\\x95\\xb4\\xe4\\xbd\\x93\\xe5\\x85\\xb3\\xe7\\xb3\\xbb\\xef\\xbc\\x88</span><span style=\"font-family: Consolas; background-color: yellow;\">components (meronyms) holonyms</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xef\\xbc\\x89</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet <span style=\"color: blue;\">as<span style=\"color: black;\"> wn</span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; wn.synset(<span style=\"color: #00aa00;\"><em>\\'tree.n.01\\'</em><span style=\"color: black;\">).part_meronyms()</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[Synset(<span style=\"color: #00aa00;\"><em>\\'burl.n.02\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'crown.n.07\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'limb.n.02\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'stump.n.01\\'</em><span style=\"color: black;\">), Synset(<span style=\"color: #00aa00;\"><em>\\'trunk.n.01\\'</em><span style=\"color: black;\">)] </span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xe5\\x8f\\x8d\\xe4\\xb9\\x89\\xe8\\xaf\\x8d\\xe5\\x85\\xb3\\xe7\\xb3\\xbb</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; wn.lemma(<span style=\"color: #00aa00;\"><em>\\'beautiful.a.01.beautiful\\'</em><span style=\"color: black;\">).antonyms()</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[Lemma(<span style=\"color: #00aa00;\"><em>\\'ugly.a.01.ugly\\'</em><span style=\"color: black;\">)] </span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas; background-color: yellow;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\">\\xe5\\x90\\x8c\\xe4\\xb9\\x89\\xe8\\xaf\\x8d\\xe5\\x85\\xb3\\xe7\\xb3\\xbb</span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; background-color: yellow;\"># \\xe6\\x9f\\xa5\\xe7\\x9c\\x8b\\xe8\\xaf\\x8d\\xe6\\xb1\\x87\\xe5\\x85\\xb3\\xe7\\xb3\\xbb\\xe5\\x92\\x8c\\xe5\\x90\\x8c\\xe4\\xb9\\x89\\xe8\\xaf\\x8d\\xe9\\x9b\\x86\\xe4\\xb8\\x8a\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe5\\x85\\xb6\\xe5\\xae\\x83\\xe6\\x96\\xb9\\xe6\\xb3\\x95</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; dir(wn.synset(<span style=\"color: #00aa00;\"><em>\\'beautiful.a.01\\'</em><span style=\"color: black;\">))</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'__class__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__delattr__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__dict__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__doc__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__eq__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__format__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__ge__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__getattribute__\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'__gt__\\'</em><span style=\"color: black;\"> ... <span style=\"color: #00aa00;\"><em>\\'substance_holonyms\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'substance_meronyms\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'topic_domains\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'tree\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'unicode_repr\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'usage_domains\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'verb_groups\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'wup_similarity\\'</em><span style=\"color: black;\">]</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-family: Consolas; font-size: 12pt;\"><span style=\"background-color: yellow;\"># Part-of-Speech (POS)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; syn = wordnet.synsets(<span style=\"color: #00aa00;\"><em>\\'motorcar\\'</em><span style=\"color: black;\">)[<span style=\"color: maroon;\">0<span style=\"color: black;\">]</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; syn.pos<span style=\"background-color: yellow;\">()</span></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'n\\'</em></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Looking up lemmas and synonyms in WordNet</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xe4\\xb8\\x80</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet <span style=\"color: blue;\">as<span style=\"color: black;\"> wn</span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; wn.synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">).lemma_names()</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">[u<span style=\"color: #00aa00;\"><em>\\'car\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'auto\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'automobile\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'machine\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'motorcar\\'</em><span style=\"color: black;\">] #</span></span></span></span></span></span></span></span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xbb\\x93\\xe6\\x9e\\x9c\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe4\\xb8\\xb2\\xe6\\x9c\\x89\\xe4\\xb8\\x80\\xe4\\xb8\\xaa</span><span style=\"font-family: Consolas;\">u </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x89\\x8d\\xe7\\xbc\\x80\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\xe5\\xae\\x83\\xe4\\xbb\\xac\\xe6\\x98\\xaf</span><span style=\"font-family: Consolas;\">Unicode </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe4\\xb8\\xb2</span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; u<span style=\"color: #00aa00;\"><em>\\'motorcar\\'</em><span style=\"color: black;\">.encode(<span style=\"color: #00aa00;\"><em>\\'utf-8\\'</em><span style=\"color: black;\">)</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: #00aa00; font-family: Consolas; font-size: 12pt;\"><em>\\'motorcar\\'</em></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xe4\\xba\\x8c</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; a = wn.synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">).lemma_names()</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> a</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[u<span style=\"color: #00aa00;\"><em>\\'car\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'auto\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'automobile\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'machine\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'motorcar\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; wn.synset(<span style=\"color: #00aa00;\"><em>\\'car.n.01\\'</em><span style=\"color: black;\">).definition ()</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'a motor vehicle with four wheels; usually propelled by an internal combustion engine\\'</em></span> </span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Calculating WordNet synset similarity</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe9\\x87\\x8f\\xe5\\xba\\xa6\\xe4\\xb8\\x80\\xef\\xbc\\x9a</span><span style=\"font-family: Consolas;\">path_similarity--</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x9f\\xba\\xe4\\xba\\x8e\\xe4\\xb8\\x8a\\xe4\\xbd\\x8d\\xe8\\xaf\\x8d\\xe5\\xb1\\x82\\xe6\\xac\\xa1\\xe7\\xbb\\x93\\xe6\\x9e\\x84\\xe4\\xb8\\xad\\xe7\\x9b\\xb8\\xe4\\xba\\x92\\xe8\\xbf\\x9e\\xe6\\x8e\\xa5\\xe7\\x9a\\x84\\xe6\\xa6\\x82\\xe5\\xbf\\xb5\\xe4\\xb9\\x8b\\xe9\\x97\\xb4\\xe7\\x9a\\x84\\xe6\\x9c\\x80\\xe7\\x9f\\xad\\xe8\\xb7\\xaf\\xe5\\xbe\\x84\\xe5\\x9c\\xa8</span><span style=\"font-family: Consolas;\">0-1</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb9\\x8b\\xe9\\x97\\xb4\\xe6\\x89\\x93\\xe5\\x88\\x86\\xef\\xbc\\x88\\xe4\\xb8\\xa4\\xe8\\x80\\x85\\xe4\\xb9\\x8b\\xe9\\x97\\xb4\\xe6\\xb2\\xa1\\xe6\\x9c\\x89\\xe8\\xb7\\xaf\\xe5\\xbe\\x84\\xe8\\xbf\\x94\\xe5\\x9b\\x9e</span><span style=\"font-family: Consolas;\">-1</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x8c\\xe4\\xb8\\x8e\\xe8\\x87\\xaa\\xe8\\xba\\xab\\xe6\\xaf\\x94\\xe8\\xbe\\x83\\xe8\\xbf\\x94\\xe5\\x9b\\x9e</span><span style=\"font-family: Consolas;\">1</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x89</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet <span style=\"color: blue;\">as<span style=\"color: black;\"> wn</span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; right = wn.synset(<span style=\"color: #00aa00;\"><em>\\'right_whale.n.01\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; minke = wn.synset(<span style=\"color: #00aa00;\"><em>\\'minke_whale.n.01\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; right.path_similarity(minke)</span> </span></p>\\r\\n<p><span style=\"color: maroon; font-family: Consolas; font-size: 12pt;\">0.25 </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe9\\x87\\x8f\\xe5\\xba\\xa6\\xe4\\xba\\x8c\\xef\\xbc\\x9a</span><span style=\"font-family: Consolas;\">wup_similarity -- wup_similarity is short for Wu-Palmer Similarity, which is a scoring method based on how similar the word senses are and where the synsets occur relative to each other in the </span></span></p>\\r\\n<p><span style=\"color: red; font-family: Consolas; font-size: 12pt;\">hypernym tree. </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> wordnet</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; cb = wordnet.synset(<span style=\"color: #00aa00;\"><em>\\'cookbook.n.01\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; ib = wordnet.synset(<span style=\"color: #00aa00;\"><em>\\'instruction_book.n.01\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; cb.wup_similarity(ib)</span> </span></p>\\r\\n<p><span style=\"color: maroon; font-family: Consolas; font-size: 12pt;\">0.9166666666666666 </span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Discovering word collocations (relative to n-gram)</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk <span style=\"color: blue;\">import<span style=\"color: black;\"> bigrams</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; a = <span style=\"color: #00aa00;\"><em>\"Jaganadh is testing this application\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tokens = a.split()</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; bigrams(tokens)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(<span style=\"color: #00aa00;\"><em>\\'Jaganadh\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'testing\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'testing\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'this\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'this\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'application.\\'</em><span style=\"color: black;\">)]</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe5\\xb7\\xb2\\xe5\\x88\\x86\\xe8\\xaf\\x8d\\xef\\xbc\\x8c\\xe5\\x88\\x99\\xef\\xbc\\x9a</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; bigrams([<span style=\"color: #00aa00;\"><em>\\'more\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'said\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'than\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'done\\'</em><span style=\"color: black;\">])</span> </span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(<span style=\"color: #00aa00;\"><em>\\'more\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'said\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'said\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'than\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'than\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'done\\'</em><span style=\"color: black;\">)]</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; font-size: 12pt;\"><strong>\\xe8\\xaf\\x8d\\xe9\\xa2\\x91\\xe7\\xbb\\x9f\\xe8\\xae\\xa1</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.book <span style=\"color: blue;\">import<span style=\"color: black;\"> text1</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">*** Introductory Examples <span style=\"color: blue;\">for<span style=\"color: black;\"> the NLTK Book ***</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Loading text1, ..., text9 <span style=\"color: blue;\">and<span style=\"color: black;\"> sent1, ..., sent9</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Type the name of the text <span style=\"color: blue;\">or<span style=\"color: black;\"> sentence to view it.</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Type: <span style=\"color: #00aa00;\"><em>\\'texts()\\'</em><span style=\"color: black;\"> <span style=\"color: blue;\">or<span style=\"color: black;\"> <span style=\"color: #00aa00;\"><em>\\'sents()\\'</em><span style=\"color: black;\"> to list the materials.</span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text1: Moby Dick by Herman Melville <span style=\"color: maroon;\">1851</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text2: Sense <span style=\"color: blue;\">and<span style=\"color: black;\"> Sensibility by Jane Austen <span style=\"color: maroon;\">1811</span> </span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">text3: The Book of Genesis</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">text4: Inaugural Address Corpus</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">text5: Chat Corpus</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text6: Monty Python <span style=\"color: blue;\">and<span style=\"color: black;\"> the Holy Grail</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">text7: Wall Street Journal</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">text8: Personals Corpus</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text9: The Man Who Was Thursday by G . K . Chesterton <span style=\"color: maroon;\">1908</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk <span style=\"color: blue;\">import<span style=\"color: black;\"> FreqDist</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; fdist1 = FreqDist(text1)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> fdist1</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&lt;FreqDist <span style=\"color: blue;\">with<span style=\"color: black;\"> <span style=\"color: maroon;\">19317<span style=\"color: black;\"> samples <span style=\"color: blue;\">and<span style=\"color: black;\"> <span style=\"color: maroon;\">260819<span style=\"color: black;\"> outcomes&gt;</span> </span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; fdist1</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">FreqDist({u<span style=\"color: #00aa00;\"><em>\\',\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">18713<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'the\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">13721<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">6862<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'of\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">6536<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'and\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">6024<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'a\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">4569<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'to\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">4542<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\';\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">4072<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'in\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">3916<span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'that\\'</em><span style=\"color: black;\">: <span style=\"color: maroon;\">2982<span style=\"color: black;\">, ...})</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; fdist1.most_common(<span style=\"color: maroon;\">50<span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(u<span style=\"color: #00aa00;\"><em>\\',\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">18713<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'the\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">13721<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">6862<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'of\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">6536<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'and\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">6024<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'a\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">4569<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'to\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">4542<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\';\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">4072<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'in\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">3916<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'that\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2982<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\"\\'\"</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2684<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'-\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2552<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'his\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2459<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'it\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2209<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'I\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">2124<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'s\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1739<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'is\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1695<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'he\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1661<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'with\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1659<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'was\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1632<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'as\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1620<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'\"\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1478<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'all\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1462<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'for\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1414<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'this\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1280<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'!\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1269<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'at\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1231<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'by\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1137<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'but\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1113<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'not\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1103<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'--\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1070<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'him\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1058<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'from\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1052<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'be\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1030<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'on\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">1005<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'so\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">918<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'whale\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">906<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'one\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">889<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'you\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">841<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'had\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">767<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'have\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">760<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'there\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">715<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'But\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">705<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'or\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">697<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'were\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">680<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'now\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">646<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'which\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">640<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'?\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">637<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'me\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">627<span style=\"color: black;\">), (u<span style=\"color: #00aa00;\"><em>\\'like\\'</em><span style=\"color: black;\">, <span style=\"color: maroon;\">624<span style=\"color: black;\">)] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xbb\\x98\\xe5\\x88\\xb6\\xe7\\xb4\\xaf\\xe7\\xa7\\xaf\\xe9\\xa2\\x91\\xe7\\x8e\\x87\\xe5\\x9b\\xbe</span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> matplotlib <span style=\"color: red;\"># </span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe7\\x9b\\xb4\\xe6\\x8e\\xa5\\xe8\\xbf\\x90\\xe8\\xa1\\x8c</span><span style=\"color: red;\">from matplotlib import plot\\xe3\\x80\\x82</span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; fdist1.plot(<span style=\"color: maroon;\">50<span style=\"color: black;\">, cumulative=<span style=\"color: blue;\">True<span style=\"color: black;\">) <span style=\"color: red;\"># </span></span></span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8f\\xaf\\xe8\\x83\\xbd\\xe5\\xa4\\x84\\xe7\\x90\\x86\\xe7\\x9a\\x84\\xe6\\x97\\xb6\\xe9\\x97\\xb4\\xe6\\xaf\\x94\\xe8\\xbe\\x83\\xe9\\x95\\xbf\\xe3\\x80\\x82</span></span></p>\\r\\n<p><img src=\"https://images2018.cnblogs.com/blog/1220982/201802/1220982-20180223235241556-1268527263.png\" alt=\"\" /></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Stemming words</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8d\\x95\\xe4\\xb8\\xaa</span></span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.stem <span style=\"color: blue;\">import<span style=\"color: black;\"> PorterStemmer <span style=\"color: red;\"># </span></span></span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"</span><span style=\"color: red;\"><span style=\"font-family: Consolas;\">Poter</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe7\\xa7\\x8d\\xe8\\xaf\\x8d\\xe5\\xb9\\xb2\\xe6\\x8f\\x90\\xe5\\x8f\\x96\\xe7\\x9a\\x84\\xe7\\xae\\x97\\xe6\\xb3\\x95\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; stemmer = PorterStemmer()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; stemmer.stem(<span style=\"color: #00aa00;\"><em>\\'cooking\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'cook\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; stemmer.stem(<span style=\"color: #00aa00;\"><em>\\'cookery\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'cookeri\\'</em><span style=\"color: black;\"> <span style=\"color: red;\"># The resulting stem is not always a valid word. For example, the</span> </span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">stem of <span style=\"color: #00aa00;\"><em>\"cookery\"</em><span style=\"color: black;\"> <span style=\"color: blue;\">is<span style=\"color: black;\"> <span style=\"color: #00aa00;\"><em>\"cookeri\"</em><span style=\"color: black;\">. This <span style=\"color: blue;\">is<span style=\"color: black;\"> a feature, <span style=\"color: blue;\">not<span style=\"color: black;\"> a bug.</span> </span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xa4\\x9a\\xe4\\xb8\\xaa</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> nltk</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; stemmer = nltk.PorterStemmer()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; verbs = [<span style=\"color: #00aa00;\"><em>\\'appears\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'appear\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'appeared\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'calling\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'called\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; stems = []</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">for<span style=\"color: black;\"> verb <span style=\"color: blue;\">in<span style=\"color: black;\"> verbs:</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> stemmed_verb = stemmer.stem(verb)</span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\"> stems.append(stemmed_verb) <span style=\"color: red;\"># </span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x80<span style=\"color: red;\">\\xe5\\xae\\x9a\\xe8\\xa6\\x81\\xe6\\x8c\\x89\\xe4\\xb8\\xa4\\xe6\\xac\\xa1\\xe5\\x9b\\x9e\\xe8\\xbd\\xa6\\xe9\\x94\\xae\\xef\\xbc\\x8c\\xe7\\x84\\xb6\\xe5\\x90\\x8e\\xe5\\x86\\x8d\\xe8\\xbe\\x93\\xe5\\x85\\xa5\\xe4\\xb8\\x8b\\xe9\\x9d\\xa2\\xe7\\x9a\\x84\\xe8\\xaf\\xad\\xe5\\x8f\\xa5\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; sorted(set(stems))</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[u<span style=\"color: #00aa00;\"><em>\\'appear\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'call\\'</em><span style=\"color: black;\">] </span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Lemmatizing words with WordNet</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.stem <span style=\"color: blue;\">import<span style=\"color: black;\"> WordNetLemmatizer</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; lemmatizer = WordNetLemmatizer()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; lemmatizer.lemmatize(<span style=\"color: #00aa00;\"><em>\\'cooking\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'cooking\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; lemmatizer.lemmatize(<span style=\"color: #00aa00;\"><em>\\'cooking\\'</em><span style=\"color: black;\">, pos=<span style=\"color: #00aa00;\"><em>\\'v\\'</em><span style=\"color: black;\">)</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'cook\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; lemmatizer.lemmatize(<span style=\"color: #00aa00;\"><em>\\'cookbooks\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">u<span style=\"color: #00aa00;\"><em>\\'cookbook\\'</em></span> </span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Replacing words matching regular expressions</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xac\\xac\\xe4\\xb8\\x80\\xe6\\xad\\xa5\\xef\\xbc\\x9a\\xe6\\x96\\xb0\\xe5\\xbb\\xba\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe5\\x90\\x8d\\xe4\\xb8\\xba</span><span style=\"font-family: Consolas;\">\"replacers.py\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x9a\\x84\\xe6\\xa8\\xa1\\xe5\\x9d\\x97</span><span style=\"font-family: Consolas;\">(</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe5\\x9c\\xa8\\xe6\\x8e\\xa7\\xe5\\x88\\xb6\\xe5\\x8f\\xb0\\xe4\\xb8\\x8a\\xe6\\x93\\x8d\\xe4\\xbd\\x9c</span><span style=\"font-family: Consolas;\">)</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xef\\xbc\\x8c\\xe6\\x94\\xbe\\xe7\\xbd\\xae\\xe4\\xba\\x8e\\xe5\\xae\\x89\\xe8\\xa3\\x85</span><span style=\"font-family: Consolas;\">Python</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x9a\\x84\\xe7\\x9b\\xae\\xe5\\xbd\\x95\\xe4\\xb8\\x8b\\xe7\\x9a\\x84</span><span style=\"font-family: Consolas;\">\"Lib\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xe5\\xa4\\xb9\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\x8f\\xaf\\xe7\\xbd\\xae\\xe4\\xba\\x8e\\xe5\\xae\\x89\\xe8\\xa3\\x85</span><span style=\"font-family: Consolas;\">Python</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x9a\\x84\\xe6\\xa0\\xb9\\xe7\\x9b\\xae\\xe5\\xbd\\x95\\xe4\\xb8\\x8b\\xef\\xbc\\x8c\\xe4\\xbd\\x86\\xe6\\x9c\\x80\\xe5\\xa5\\xbd\\xe6\\x94\\xbe\\xe5\\x9c\\xa8</span><span style=\"font-family: Consolas;\">\"Lib\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x8b\\xef\\xbc\\x8c\\xe4\\xbb\\xa5\\xe8\\xa1\\xa8\\xe6\\x98\\x8e\\xe8\\xbf\\x99\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe5\\xba\\x93\\xef\\xbc\\x88\\xe5\\x8d\\xb3</span><span style=\"font-family: Consolas;\">Python</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\xad\\xe7\\x9a\\x84\\xe6\\xa8\\xa1\\xe5\\x9d\\x97\\xef\\xbc\\x89\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"color: blue; font-family: Consolas; font-size: 12pt;\">import<span style=\"color: black;\"> re</span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">replacement_patterns = [</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'won\\\\\\'t\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'will not\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'can\\\\\\'t\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'cannot\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'i\\\\\\'m\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'i am\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'ain\\\\\\'t\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'is not\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)\\\\\\'ll\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; will\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)n\\\\\\'t\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; not\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)\\\\\\'ve\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; have\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)\\\\\\'s\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; is\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)\\\\\\'re\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; are\\'</em><span style=\"color: black;\">),</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (r<span style=\"color: #00aa00;\"><em>\\'(\\\\w+)\\\\\\'d\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'\\\\g&lt;1&gt; would\\'</em><span style=\"color: black;\">)</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">]</span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: blue; font-family: Consolas; font-size: 12pt;\">class<span style=\"color: black;\"> <strong>RegexpReplacer</strong>(object):</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <span style=\"color: blue;\">def<span style=\"color: black;\"> <strong>__init__</strong>(<em>self</em>, patterns=replacement_patterns):</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <em>self</em>.patterns = [(re.compile(regex), repl) <span style=\"color: blue;\">for<span style=\"color: black;\"> (regex, repl) <span style=\"color: blue;\">in<span style=\"color: black;\"> patterns]</span> </span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <span style=\"color: blue;\">def<span style=\"color: black;\"> <strong>replace</strong>(<em>self</em>, text):</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> s = text</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <span style=\"color: blue;\">for<span style=\"color: black;\"> (pattern, repl) <span style=\"color: blue;\">in<span style=\"color: black;\"> <em>self</em>.patterns:</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> s = re.sub(pattern, repl, s)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <span style=\"color: blue;\">return<span style=\"color: black;\"> s</span> </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xac\\xac\\xe4\\xba\\x8c\\xe6\\xad\\xa5</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> replacers <span style=\"color: blue;\">import<span style=\"color: black;\"> RegexpReplacer</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; replacer = RegexpReplacer()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; replacer.replace(<span style=\"color: #00aa00;\"><em>\"can\\'t is a contraction\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'cannot is a contraction\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; replacer.replace(<span style=\"color: #00aa00;\"><em>\"I should\\'ve done that thing I didn\\'t do\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: #00aa00; font-family: Consolas; font-size: 12pt;\"><em>\\'I should have done that thing I did not do\\'</em></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Accessing Corpora</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> gutenberg</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">for<span style=\"color: black;\"> filename <span style=\"color: blue;\">in<span style=\"color: black;\"> gutenberg.fileids():</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> r = gutenberg.raw(filename)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> w = gutenberg.words(filename)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> s = gutenberg.sents(filename)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> v = set(w)</span> </span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black;\"><span style=\"font-family: Consolas;\"> <span style=\"color: blue;\">print<span style=\"color: black;\"> filename, len(r)/len(w), len(w)/len(s), len(w)/len(v) <span style=\"color: red;\"># </span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xa6\\x81\\xe6\\x8c\\x89\\xe4\\xb8\\xa4\\xe6\\xac\\xa1\\xe5\\x9b\\x9e\\xe8\\xbd\\xa6\\xe9\\x94\\xae\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe6\\x98\\xbe\\xe7\\xa4\\xba\\xe7\\xbb\\x93\\xe6\\x9e\\x9c\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">austen-emma.txt <span style=\"color: maroon;\">4<span style=\"color: black;\"> <span style=\"color: maroon;\">24<span style=\"color: black;\"> <span style=\"color: maroon;\">24 <span style=\"color: red;\">#</span></span></span></span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xaf\\xad\\xe6\\x96\\x99\\xe5\\xba\\x93\\xe7\\x9a\\x84\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xe5\\x90\\x8d<span style=\"color: red;\">\\xef\\xbc\\x8c\\xe5\\xb9\\xb3\\xe5\\x9d\\x87\\xe5\\xad\\x97\\xe9\\x95\\xbf\\xef\\xbc\\x8c\\xe5\\xb9\\xb3\\xe5\\x9d\\x87\\xe5\\x8f\\xa5\\xe9\\x95\\xbf\\xef\\xbc\\x8c\\xe6\\xaf\\x8f\\xe4\\xb8\\xaa\\xe8\\xaf\\x8d\\xe5\\xb9\\xb3\\xe5\\x9d\\x87\\xe5\\x87\\xba\\xe7\\x8e\\xb0\\xe7\\x9a\\x84\\xe6\\xac\\xa1\\xe6\\x95\\xb0\\xef\\xbc\\x8c\\xe4\\xb8\\x8b\\xe5\\x90\\x8c\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">austen-persuasion.txt <span style=\"color: maroon;\">4<span style=\"color: black;\"> <span style=\"color: maroon;\">26<span style=\"color: black;\"> <span style=\"color: maroon;\">16</span> </span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">austen-sense.txt <span style=\"color: maroon;\">4<span style=\"color: black;\"> <span style=\"color: maroon;\">28<span style=\"color: black;\"> <span style=\"color: maroon;\">20</span> </span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">shakespeare-macbeth.txt <span style=\"color: maroon;\">4<span style=\"color: black;\"> <span style=\"color: maroon;\">12<span style=\"color: black;\"> <span style=\"color: maroon;\">5</span> </span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">whitman-leaves.txt <span style=\"color: maroon;\">4<span style=\"color: black;\"> <span style=\"color: maroon;\">36<span style=\"color: black;\"> <span style=\"color: maroon;\">10</span></span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">&nbsp;</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.book <span style=\"color: blue;\">import<span style=\"color: black;\"> *</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">*** Introductory Examples <span style=\"color: blue;\">for<span style=\"color: black;\"> the NLTK Book ***</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Loading text1, ..., text9 <span style=\"color: blue;\">and<span style=\"color: black;\"> sent1, ..., sent9</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Type the name of the text <span style=\"color: blue;\">or<span style=\"color: black;\"> sentence to view it.</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Type: <span style=\"color: #00aa00;\"><em>\\'texts()\\'</em><span style=\"color: black;\"> <span style=\"color: blue;\">or<span style=\"color: black;\"> <span style=\"color: #00aa00;\"><em>\\'sents()\\'</em><span style=\"color: black;\"> to list the materials.</span> </span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text1: Moby Dick by Herman Melville <span style=\"color: maroon;\">1851</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">...</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">...</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">text9: The Man Who Was Thursday by G . K . Chesterton <span style=\"color: maroon;\">1908 </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; text1.concordance(<span style=\"color: #00aa00;\"><em>\"monstrous\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Displaying <span style=\"color: maroon;\">11<span style=\"color: black;\"> of <span style=\"color: maroon;\">11<span style=\"color: black;\"> matches:</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">ong the former , one was of a most monstrous size . ... This came towards us , </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">ON OF THE PSALMS . <em>\" Touching that monstrous bulk of the whale or ork we have r</em> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">ght have been rummaged out of this monstrous cabinet there is no telling . But </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u </span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93; font-size: 12pt;\"><strong>\\xe5\\xa4\\x96\\xe9\\x83\\xa8\\xe6\\x96\\x87\\xe6\\xa1\\xa3\\xe6\\x93\\x8d\\xe4\\xbd\\x9c</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xa4\\x96\\xe9\\x83\\xa8\\xe6\\x96\\x87\\xe6\\xa1\\xa3\\xe6\\x93\\x8d\\xe4\\xbd\\x9c</span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xaf\\xbb\\xe5\\x8f\\x96\\xe4\\xb8\\x80\\xe4\\xb8\\xaa</span><span style=\"font-family: Consolas;\">txt</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xef\\xbc\\x88\\xe5\\xb7\\xb2\\xe7\\xbb\\x8f\\xe6\\x96\\xb0\\xe5\\xbb\\xba\\xe4\\xb8\\x80\\xe4\\xb8\\xaa</span><span style=\"font-family: Consolas;\">\"good.txt\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xe5\\x9c\\xa8</span><span style=\"font-family: Consolas;\">D:/Python27</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe4\\xb8\\x8b\\xef\\xbc\\x89</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; f = open(<span style=\"color: #00aa00;\"><em>\\'document.txt\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; raw = f.read()</span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x89\\x93\\xe5\\x8d\\xb0</span><span style=\"font-family: Consolas;\">\"good.txt\"</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x86\\x85\\xe5\\xae\\xb9\\xe3\\x80\\x82</span><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8d\\xb3\\xe4\\xbd\\xbf\\xe6\\x89\\x93\\xe5\\x8d\\xb0</span><span style=\"font-family: Consolas;\">Walden</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xbf\\x99\\xe6\\xa0\\xb7\\xe6\\xaf\\x94\\xe8\\xbe\\x83\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xe4\\xb9\\x9f\\xe6\\xaf\\x94\\xe8\\xbe\\x83\\xe5\\xbf\\xab</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; f = open (<span style=\"color: #00aa00;\"><em>\\'good.txt\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> f.read()</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">happy </span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">Lucy</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Lilei </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: red;\"># -*- coding:utf-8 -*-</span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; f = open (<span style=\"color: #00aa00;\"><em>\\'</em></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\"><em>\\xe8\\xaf\\xad\\xe8\\xa8\\x80\\xe5\\xad\\xa6</em></span><span style=\"color: #00aa00; font-family: Consolas;\"><em>.txt\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> f.read()</span> </span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black; font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xae\\xa1\\xe7\\xae\\x97\\xe8\\xaf\\xad\\xe8\\xa8\\x80\\xe5\\xad\\xa6</span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black; font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\x87\\xaa\\xe7\\x84\\xb6\\xe8\\xaf\\xad\\xe8\\xa8\\x80\\xe5\\xa4\\x84\\xe7\\x90\\x86</span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xbb\\xba\\xe7\\xab\\x8b\\xe8\\x87\\xaa\\xe5\\xb7\\xb1\\xe7\\x9a\\x84\\xe8\\xaf\\xad\\xe6\\x96\\x99\\xe5\\xba\\x93\\xef\\xbc\\x8c\\xe5\\xb9\\xb6\\xe5\\xaf\\xb9\\xe8\\xaf\\xad\\xe6\\x96\\x99\\xe5\\xba\\x93\\xe9\\x87\\x8c\\xe7\\x9a\\x84\\xe6\\x96\\x87\\xe4\\xbb\\xb6\\xe8\\xbf\\x9b\\xe8\\xa1\\x8c\\xe6\\xa3\\x80\\xe7\\xb4\\xa2</span></span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xac\\xac\\xe4\\xb8\\x80\\xe6\\xad\\xa5</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; corpus_root = <span style=\"color: #00aa00;\"><em>\\'D:/Python27/my own data\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> PlaintextCorpusReader</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; corpus_root = <span style=\"color: #00aa00;\"><em>\\'D:/Python27/my own data\\'</em></span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; wordlists = PlaintextCorpusReader(corpus_root, <span style=\"color: #00aa00;\"><em>\\'Walden.txt\\'</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; wordlists.fileids()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Walden.txt\\'</em><span style=\"color: black;\">]</span> </span></span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\">#</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\xb3\\xa8\\xe6\\x84\\x8f\\xef\\xbc\\x9a</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.corpus <span style=\"color: blue;\">import<span style=\"color: black;\"> PlaintextCorpusReader</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; corpus_root = <span style=\"color: #00aa00;\"><em>\\'D:/Python27/my own data\\'</em></span> </span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; wordlists = PlaintextCorpusReader(corpus_root, <span style=\"color: #00aa00;\"><em>\\'.*\\'</em><span style=\"color: black;\">) <span style=\"color: red;\"># .*</span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x85\\xb7\\xe6\\x9c\\x89\\xe8\\xb4\\xaa\\xe5\\xa9\\xaa\\xe7\\x9a\\x84\\xe6\\x80\\xa7\\xe8\\xb4\\xa8\\xef\\xbc\\x8c\\xe9\\xa6\\x96\\xe5\\x85\\x88\\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe5\\x88\\xb0\\xe4\\xb8\\x8d\\xe8\\x83\\xbd\\xe5\\x8c\\xb9\\xe9\\x85\\x8d\\xe4\\xb8\\xba\\xe6\\xad\\xa2\\xef\\xbc\\x8c\\xe6\\xa0\\xb9\\xe6\\x8d\\xae\\xe5\\x90\\x8e\\xe9\\x9d\\xa2\\xe7\\x9a\\x84\\xe6\\xad\\xa3\\xe5\\x88\\x99\\xe8\\xa1\\xa8\\xe8\\xbe\\xbe\\xe5\\xbc\\x8f\\xef\\xbc\\x8c\\xe4\\xbc\\x9a\\xe8\\xbf\\x9b\\xe8\\xa1\\x8c\\xe5\\x9b\\x9e\\xe6\\xba\\xaf\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; wordlists.fileids()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'Gone with the Wind.txt\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'Walden.txt\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xac\\xac\\xe4\\xba\\x8c\\xe6\\xad\\xa5</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; n = nltk.word_tokenize(wordlists.raw(fileids=<span style=\"color: #00aa00;\"><em>\"Walden.txt\"</em><span style=\"color: black;\">))</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; complete_Walden = nltk.Text(n)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; complete_Walden.concordance(<span style=\"color: #00aa00;\"><em>\"love\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">Displaying <span style=\"color: maroon;\">25<span style=\"color: black;\"> of <span style=\"color: maroon;\">40<span style=\"color: black;\"> matches:</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">r even to found a school , but so to love wisdom as to live according to its d </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> , perhaps we are led oftener by the love of novelty and a regard for the opin </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">... </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">eed have you to employ punishments ? Love virtue , and the people will be virt </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">abardine dressed . <em>\\'\\'</em> <strong>``</strong> Come ye who love , And ye who hate , Children of the </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\x8e\\xb7\\xe5\\x8f\\x96\\xe7\\xbd\\x91\\xe7\\xbb\\x9c\\xe6\\x96\\x87\\xe6\\x9c\\xac</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; from urllib import urlopen </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; url = <span style=\"color: #00aa00;\"><em>\"http://news.bbc.co.uk/2/hi/health/2284783.stm\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; html = urlopen(url).read()</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; html[:<span style=\"color: maroon;\">60<span style=\"color: black;\">]</span> </span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\\'&lt;!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\\'</em></span> </span></p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x8e\\xa5\\xe4\\xb8\\x8b\\xe6\\x9d\\xa5\\xe5\\xa6\\x82\\xe6\\x9e\\x9c\\xe8\\xbe\\x93\\xe5\\x85\\xa5</span><span style=\"font-family: Consolas;\">print html</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe7\\x9c\\x8b\\xe5\\x88\\xb0</span><span style=\"font-family: Consolas;\">HTML </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x9a\\x84\\xe5\\x85\\xa8\\xe9\\x83\\xa8\\xe5\\x86\\x85\\xe5\\xae\\xb9\\xef\\xbc\\x8c</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8c\\x85\\xe6\\x8b\\xac</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">meta </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x85\\x83\\xe6\\xa0\\x87\\xe7\\xad\\xbe\\xe3\\x80\\x81\\xe5\\x9b\\xbe\\xe5\\x83\\x8f\\xe6\\xa0\\x87\\xe7\\xad\\xbe\\xe3\\x80\\x81</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">map </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\xa0\\x87 </span></span></p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\xad\\xbe\\xe3\\x80\\x81</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">JavaScript</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe3\\x80\\x81\\xe8\\xa1\\xa8\\xe5\\x8d\\x95\\xe5\\x92\\x8c\\xe8\\xa1\\xa8\\xe6\\xa0\\xbc\\xe3\\x80\\x82</span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: red;\"><span style=\"font-family: Consolas;\">#</span> </span>NLTK\\xe6\\x9c\\xac\\xe6\\x9d\\xa5\\xe6\\x8f\\x90\\xe4\\xbe\\x9b\\xe4\\xba\\x86\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe8\\xbe\\x85\\xe5\\x8a\\xa9\\xe5\\x87\\xbd\\xe6\\x95\\xb0nltk.clean_html()\\xe5\\xb0\\x86HTML \\xe5\\xad\\x97\\xe7\\xac\\xa6\\xe4\\xb8\\xb2\\xe4\\xbd\\x9c\\xe4\\xb8\\xba\\xe5\\x8f\\x82\\xe6\\x95\\xb0\\xef\\xbc\\x8c\\xe8\\xbf\\x94\\xe5\\x9b\\x9e\\xe5\\x8e\\x9f\\xe5\\xa7\\x8b\\xe6\\x96\\x87\\xe6\\x9c\\xac\\xef\\xbc\\x9b\\xe4\\xbd\\x86\\xe7\\x8e\\xb0\\xe5\\x9c\\xa8\\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe5\\x87\\xbd\\xe6\\x95\\xb0\\xe5\\xb7\\xb2\\xe7\\xbb\\x8f\\xe4\\xb8\\x8d\\xe8\\xa2\\xab\\xe6\\x94\\xaf\\xe6\\x8c\\x81\\xe4\\xba\\x86\\xef\\xbc\\x8c\\xe8\\x80\\x8c\\xe6\\x98\\xaf\\xe7\\x94\\xa8BeautifulSoup\\xe7\\x9a\\x84\\xe5\\x87\\xbd\\xe6\\x95\\xb0get_text()\\xe3\\x80\\x82 </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> urllib</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> bs4 <span style=\"color: blue;\">import<span style=\"color: black;\"> BeautifulSoup</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; url = <span style=\"color: #00aa00;\"><em>\"http://news.bbc.co.uk/2/hi/health/2284783.stm\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; html = urllib.urlopen(url).read()</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; soup = BeautifulSoup(html)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> soup.get_text() </span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xaf\\xb9\\xe7\\xbd\\x91\\xe7\\xbb\\x9c\\xe6\\x96\\x87\\xe6\\x9c\\xac\\xe5\\x88\\x86\\xe8\\xaf\\x8d</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> urllib</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> bs4 <span style=\"color: blue;\">import<span style=\"color: black;\"> BeautifulSoup</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; url = <span style=\"color: #00aa00;\"><em>\"http://news.bbc.co.uk/2/hi/health/2284783.stm\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; html = urllib.urlopen(url).read()</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; soup = BeautifulSoup(html)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; raw = BeautifulSoup.get_text(soup)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">from<span style=\"color: black;\"> nltk.tokenize <span style=\"color: blue;\">import<span style=\"color: black;\"> word_tokenize</span> </span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; token = nltk.word_tokenize(raw)</span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; token <span style=\"color: red;\"># </span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe5\\x9c\\xa8</span><span style=\"color: red;\"><span style=\"font-family: Consolas;\">token</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\x89\\x8d\\xe5\\x8a\\xa0\\xe4\\xb8\\x8a\"</span><span style=\"font-family: Consolas;\">print</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[u<span style=\"color: #00aa00;\"><em>\\'BBC\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'NEWS\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'|\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'Health\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'|\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'Blondes\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\"\\'to\"</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'die\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'out\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'in\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'200\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\"years\\'\"</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'NEWS\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'SPORT\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'WEATHER\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'WORLD\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'SERVICE\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'A-Z\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'INDEX\\'</em><span style=\"color: black;\">, ...] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe9\\x87\\x8d\\xe7\\x82\\xb9</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; s = [u<span style=\"color: #00aa00;\"><em>\\'r118\\'</em><span style=\"color: black;\">, u<span style=\"color: #00aa00;\"><em>\\'BB\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; [str(item) <span style=\"color: blue;\">for<span style=\"color: black;\"> item <span style=\"color: blue;\">in<span style=\"color: black;\"> s]</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'r118\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'BB\\'</em><span style=\"color: black;\">]</span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Default tagging</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x89\\x88\\xe6\\x9c\\xac\\xe4\\xb8\\x80</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> nltk</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; text=nltk.word_tokenize(<span style=\"color: #00aa00;\"><em>\"We are going out.Just you and me.\"</em><span style=\"color: black;\">)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">print<span style=\"color: black;\"> nltk.pos_tag(text)</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(<span style=\"color: #00aa00;\"><em>\\'We\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'PRP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'are\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'VBP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'going\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'VBG\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'out.Just\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'JJ\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'you\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'PRP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'and\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'CC\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'me\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'PRP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">)] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe7\\x89\\x88\\xe6\\x9c\\xac\\xe4\\xba\\x8c</span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; sentence = <span style=\"color: #00aa00;\"><em>\"\"\"At eight o\\'clock on Thursday morning Arthur didn\\'t feel very good.\"\"\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tokens = nltk.word_tokenize(sentence)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tokens</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[<span style=\"color: #00aa00;\"><em>\\'At\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'eight\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"o\\'clock\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'on\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'Thursday\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'morning\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'Arthur\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'did\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"n\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'feel\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'very\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'good\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">]</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tagged = nltk.pos_tag(tokens)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; tagged</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(<span style=\"color: #00aa00;\"><em>\\'At\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'IN\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'eight\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'CD\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"o\\'clock\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'JJ\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'on\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'IN\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'Thursday\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'NNP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'morning\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'NN\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'Arthur\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'NNP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'did\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'VBD\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"n\\'t\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'RB\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'feel\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'VB\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'very\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'RB\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'good\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'JJ\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'.\\'</em><span style=\"color: black;\">)]</span> </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; tagged[<span style=\"color: maroon;\">0<span style=\"color: black;\">:<span style=\"color: maroon;\">6<span style=\"color: black;\">]</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">[(<span style=\"color: #00aa00;\"><em>\\'At\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'IN\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'eight\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'CD\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"o\\'clock\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'JJ\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'on\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'IN\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'Thursday\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'NNP\\'</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\\'morning\\'</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\\'NN\\'</em><span style=\"color: black;\">)] </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 21px; background: silver;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><strong>Chunking and chinking with regular expressions</strong></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n<tr style=\"height: 19px;\">\\r\\n<td style=\"padding-top: 1px; padding-left: 7px; padding-bottom: 1px; padding-right: 7px; border-top: none; border-left: solid black 0.5pt; border-bottom: solid black 0.5pt; border-right: solid black 0.5pt;\" valign=\"middle\">\\r\\n<p><span style=\"color: red; font-family: Consolas; font-size: 12pt;\"># Chunking &amp; Parsing </span></p>\\r\\n<p><span style=\"color: red; font-size: 12pt;\"><span style=\"font-family: Consolas;\"># Chart Parsing </span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x98\\xaf\\xe6\\x8f\\x8f\\xe8\\xbf\\xb0</span><span style=\"font-family: Consolas;\">CFG(Context Free Grammar)</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xaf\\xad\\xe6\\xb3\\x95\\xe7\\x9a\\x84\\xe4\\xb8\\x80\\xe7\\xa7\\x8d\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xef\\xbc\\x8c\\xe4\\xb8\\xa4\\xe8\\x80\\x85\\xe4\\xb8\\x8d\\xe6\\x98\\xaf\\xe5\\xb9\\xb3\\xe8\\xa1\\x8c\\xe5\\x85\\xb3\\xe7\\xb3\\xbb\\xe3\\x80\\x82</span></span></p>\\r\\n<p><span style=\"color: blue; font-family: Consolas; font-size: 12pt;\">import<span style=\"color: black;\"> nltk </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">grammar = r<span style=\"color: #00aa00;\"><em>\"\"\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>NP: {&lt;DT|PP\\\\$&gt;?&lt;JJ&gt;*&lt;NN&gt;} # chunk determiner/possessive, adjectives and nouns</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>{&lt;NNP&gt;+} # chunk sequences of proper nouns</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em>\"\"\"</em></span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">cp = nltk.RegexpParser(grammar)</span> </span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">tagged_tokens = [(<span style=\"color: #00aa00;\"><em>\"Rapunzel\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"NNP\"</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"let\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"VBD\"</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"down\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"RP\"</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"her\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"PP$\"</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"golden\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"JJ\"</em><span style=\"color: black;\">), (<span style=\"color: #00aa00;\"><em>\"hair\"</em><span style=\"color: black;\">, <span style=\"color: #00aa00;\"><em>\"NN\"</em><span style=\"color: black;\">)] <span style=\"color: red;\"># </span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe5\\xae\\x9e\\xe9\\x99\\x85\\xe8\\xbf\\x90\\xe7\\x94\\xa8\\xe4\\xb8\\xad\\xef\\xbc\\x8c\\xe9\\x9c\\x80\\xe5\\x85\\x88\\xe5\\xaf\\xb9\"</span><span style=\"color: red;\"><span style=\"font-family: Consolas;\">Rapunzel let down her golden hair.</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\"\\xe8\\xbf\\x99\\xe5\\x8f\\xa5\\xe8\\xbf\\x9b\\xe8\\xa1\\x8c</span><span style=\"font-family: Consolas;\">tokenization. </span></span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: blue; font-family: Consolas; font-size: 12pt;\">print<span style=\"color: black;\"> cp.parse(tagged_tokens) </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"color: red; font-family: Consolas; font-size: 12pt;\"># CFG Parsing</span></p>\\r\\n<p><span style=\"font-size: 12pt;\"><span style=\"color: black;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; <span style=\"color: blue;\">import<span style=\"color: black;\"> nltk <span style=\"color: red;\"># </span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xbf\\x99\\xe4\\xb8\\xa4\\xe8\\xa1\\x8c\\xe4\\xbb\\xa3\\xe7\\xa0\\x81\\xe4\\xb9\\x9f\\xe5\\x8f\\xaf<span style=\"color: red;\">\\xe4\\xbb\\xa5\\xe5\\x86\\x99\\xe6\\x88\\x90\\xef\\xbc\\x9a</span></span><span style=\"font-family: Consolas;\">from nltk import CFG; groucho_grammar = CFG.fromstring(\"\"\"</span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; groucho_grammar = nltk.CFG.fromstring(<span style=\"color: #00aa00;\"><em>\"\"\"</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> S -&gt; NP VP</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> PP -&gt; P NP</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> NP -&gt; D N | D N PP | \\'I\\'</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> VP -&gt; V NP | V PP</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> D -&gt; \\'an\\' | \\'a\\' | \\'my\\' | \\'the\\'</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> N -&gt; \\'elephant\\' | \\'pajamas\\'</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> V -&gt; \\'shot\\'</em></span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: #00aa00;\"><em> P -&gt; \\'in\\'</em></span> </span></p>\\r\\n<p><span style=\"color: #00aa00; font-family: Consolas; font-size: 12pt;\"><em> \"\"\"</em><span style=\"color: black;\">)</span> </span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; sent = <span style=\"color: #00aa00;\"><em>\"I shot an elephant in my pajamas\"</em><span style=\"color: black;\">.split() <span style=\"color: red;\"># </span></span></span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x9c\\x89\\xe5\\x8f\\xaf\\xe8\\x83\\xbd\\xe5\\xae\\x9e\\xe7\\x8e\\xb0\\xe5\\xb0\\xb1\\xe5\\x88\\x86\\xe5\\xa5\\xbd\\xe8\\xaf\\x8d\\xe4\\xba\\x86\\xef\\xbc\\x8c\\xe5\\x8d\\xb3\\xef\\xbc\\x9a</span><span style=\"color: red; font-family: Consolas;\">sent = [\\'I\\',\\'shot\\',\\'an\\',\\'elephant\\',\\'in\\',\\'my\\',\\'pajamas\\'] </span></span></p>\\r\\n<p><span style=\"color: black; font-size: 12pt;\"><span style=\"font-family: Consolas;\">&gt;&gt;&gt; parser = nltk.ChartParser(groucho_grammar) <span style=\"color: red;\"># Chart Parsing </span></span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe6\\x98\\xaf\\xe6\\x8f\\x8f\\xe8\\xbf\\xb0</span><span style=\"color: red;\"><span style=\"font-family: Consolas;\">CFG</span><span style=\"font-family: \\xe5\\xae\\x8b\\xe4\\xbd\\x93;\">\\xe8\\xaf\\xad\\xe6\\xb3\\x95\\xe7\\x9a\\x84\\xe4\\xb8\\x80\\xe7\\xa7\\x8d\\xe6\\x96\\xb9\\xe6\\xb3\\x95\\xe3\\x80\\x82</span></span></span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; all_the_parses = parser.parse(sent)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">&gt;&gt;&gt; all_the_parses</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&lt;generator object parses at <span style=\"color: maroon;\">0x030E8AD0<span style=\"color: black;\">&gt;</span> </span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\">&gt;&gt;&gt; <span style=\"color: blue;\">for<span style=\"color: black;\"> parse <span style=\"color: blue;\">in<span style=\"color: black;\"> all_the_parses:</span> </span></span></span></span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> <span style=\"color: blue;\">print<span style=\"color: black;\">(parse)</span> </span></span></p>\\r\\n<p>&nbsp;</p>\\r\\n<p>&nbsp;</p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\">(S</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> (NP I)</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> (VP</span> </span></p>\\r\\n<p><span style=\"font-family: Consolas; font-size: 12pt;\"><span style=\"color: black;\"> (V shot)</span> </span></p>\\r\\n<p><span style=\"color: black; font-family: Consolas; font-size: 12pt;\"> (NP (D an) (N elephant) (PP (P <span style=\"color: blue;\">in<span style=\"color: black;\">) (NP (D my) (N pajamas))))))</span> </span></span></p>\\r\\n</td>\\r\\n</tr>\\r\\n</tbody>\\r\\n</table>\\r\\n</div>\\r\\n<p>&nbsp;</p></div><div id=\"MySignature\"></div>\\r\\n<div class=\"clear\"></div>\\r\\n<div id=\"blog_post_info_block\">\\r\\n<div id=\"BlogPostCategory\"></div>\\r\\n<div id=\"EntryTag\"></div>\\r\\n<div id=\"blog_post_info\">\\r\\n</div>\\r\\n<div class=\"clear\"></div>\\r\\n<div id=\"post_next_prev\"></div>\\r\\n</div>\\r\\n\\r\\n\\r\\n\\t<div class = \"postDesc\"><img src=\"/skins/gray/images/speech.gif\" align=\"absmiddle\" />&nbsp;posted on <span id=\"post-date\">2018-02-01 13:01</span> <a href=\\'http://www.cnblogs.com/ArrozZhu/\\'>Arroz</a> \\xe9\\x98\\x85\\xe8\\xaf\\xbb(<span id=\"post_view_count\">...</span>) \\xe8\\xaf\\x84\\xe8\\xae\\xba(<span id=\"post_comment_count\">...</span>)  <a href =\"https://i.cnblogs.com/EditPosts.aspx?postid=8463882\" rel=\"nofollow\">\\xe7\\xbc\\x96\\xe8\\xbe\\x91</a> <a href=\"#\" onclick=\"AddToWz(8463882);return false;\">\\xe6\\x94\\xb6\\xe8\\x97\\x8f</a></div>\\r\\n</div>\\r\\n<script type=\"text/javascript\">var allowComments=false,cb_blogId=374554,cb_entryId=8463882,cb_blogApp=currentBlogApp,cb_blogUserGuid=\\'9fd47e1f-a1e3-40ce-7bfc-08d49c352df3\\',cb_entryCreatedDate=\\'2018/2/1 13:01:00\\';loadViewCount(cb_entryId);var cb_postType=1;</script>\\r\\n\\r\\n</div><a name=\"!comments\"></a><div id=\"blog-comments-placeholder\"></div><script type=\"text/javascript\">var commentManager = new blogCommentManager();commentManager.renderComments(0);</script>\\r\\n<div id=\\'comment_form\\' class=\\'commentform\\'>\\r\\n<a name=\\'commentform\\'></a>\\r\\n<div id=\\'divCommentShow\\'></div>\\r\\n<div id=\\'comment_nav\\'><span id=\\'span_refresh_tips\\'></span><a href=\\'javascript:void(0);\\' onclick=\\'return RefreshCommentList();\\' id=\\'lnk_RefreshComments\\' runat=\\'server\\' clientidmode=\\'Static\\'>\\xe5\\x88\\xb7\\xe6\\x96\\xb0\\xe8\\xaf\\x84\\xe8\\xae\\xba</a><a href=\\'#\\' onclick=\\'return RefreshPage();\\'>\\xe5\\x88\\xb7\\xe6\\x96\\xb0\\xe9\\xa1\\xb5\\xe9\\x9d\\xa2</a><a href=\\'#top\\'>\\xe8\\xbf\\x94\\xe5\\x9b\\x9e\\xe9\\xa1\\xb6\\xe9\\x83\\xa8</a></div>\\r\\n<div id=\\'comment_form_container\\'></div>\\r\\n<div class=\\'ad_text_commentbox\\' id=\\'ad_text_under_commentbox\\'></div>\\r\\n<div id=\\'ad_t2\\'></div>\\r\\n<div id=\\'opt_under_post\\'></div>\\r\\n<div id=\\'cnblogs_c1\\' class=\\'c_ad_block\\'></div>\\r\\n<div id=\\'under_post_news\\'></div>\\r\\n<div id=\\'cnblogs_c2\\' class=\\'c_ad_block\\'></div>\\r\\n<div id=\\'under_post_kb\\'></div>\\r\\n<div id=\\'HistoryToday\\' class=\\'c_ad_block\\'></div>\\r\\n<script type=\\'text/javascript\\'>\\r\\n    fixPostBody();\\r\\n    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);\\r\\n    deliverAdT2();\\r\\n    deliverAdC1();\\r\\n    deliverAdC2();    \\r\\n    loadNewsAndKb();\\r\\n    loadBlogSignature();\\r\\n    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);\\r\\n    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);\\r\\n    loadOptUnderPost();\\r\\n    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   \\r\\n</script>\\r\\n</div>\\r\\n\\r\\n\\r\\n\\t\\t</div>\\r\\n\\t</div>\\r\\n\\t</div>\\r\\n\\t<!-- left ends -->\\r\\n\\t<!-- right starts -->\\r\\n\\t<div id=\"right\">\\r\\n\\t\\t<!-- \\xe5\\x8f\\xb3\\xe4\\xbe\\xa7\\xe5\\xb7\\xa5\\xe5\\x85\\xb7\\xe9\\x83\\xa8\\xe5\\x88\\x86 -->\\r\\n\\t\\t<div id=\"right_content\">\\r\\n\\t\\r\\n\\t\\t\\t\\r\\n<!--done-->\\r\\n<h1 class=\"listtitle\" style=\"padding-top:14px;padding-left:42px;\">\\xe5\\x85\\xac\\xe5\\x91\\x8a</h1>\\r\\n<div class=\"newsItem\">\\r\\n\\t<div id=\"blog-news\"></div><script type=\"text/javascript\">loadBlogNews();</script>\\r\\n</div>\\r\\n\\r\\n\\t\\r\\n\\t\\t\\t<div id=\"calendar\"><div id=\"blog-calendar\" style=\"display:none\"></div><script type=\"text/javascript\">loadBlogDefaultCalendar();</script></div>\\r\\n\\t\\t\\t\\r\\n\\t\\t\\t\\t<DIV id=\"leftcontentcontainer\">\\r\\n\\t\\t\\t\\t\\t<div id=\"blog-sidecolumn\"></div><script type=\"text/javascript\">loadBlogSideColumn();</script>\\r\\n\\t\\t\\t\\t</DIV>\\r\\n\\t\\t\\t\\r\\n\\t\\t</div>\\r\\n\\t\\t<!-- //\\xe5\\x8f\\xb3\\xe4\\xbe\\xa7\\xe5\\xb7\\xa5\\xe5\\x85\\xb7\\xe9\\x83\\xa8\\xe5\\x88\\x86 -->\\r\\n\\t</div>\\r\\n\\t<!-- right ends -->\\r\\n\\t<div class=\"clear\"></div>\\r\\n</div>\\r\\n<div id=\"footer\">\\r\\n<!--done-->\\r\\nCopyright &copy;2018 Arroz</div>\\r\\n\\r\\n</body>\\r\\n</html>\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "print (html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取文本\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url=\"https://www.cnblogs.com/ArrozZhu/p/8463882.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "<Python Text Processing with NLTK 2.0 Cookbook>代码笔记 - Arroz - 博客园\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "var currentBlogApp = 'ArrozZhu', cb_enable_mathjax=false;var isLogined=false;\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Arroz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "随笔- 109 \r\n",
      "文章- 24 \r\n",
      "评论- 0 \r\n",
      "\r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "博客园  首页  新随笔  新文章  联系  管理  订阅 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "<Python Text Processing with NLTK 2.0 Cookbook>代码笔记\n",
      "\n",
      "如下是<Python Text Processing with NLTK 2.0 Cookbook>一书部分章节的代码笔记. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing text into sentences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> para = \"Hello World. It's good to see you. Thanks for buying this book.\" \n",
      ">>> from nltk.tokenize import sent_tokenize \n",
      ">>> sent_tokenize(para) # \"sent_tokenize\"是一个函数，下文很多中间带下划线的标识符都指的是函数。\n",
      "['Hello World.', \"It's good to see you.\", 'Thanks for buying this \n",
      "book.'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing sentences into words\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.tokenize import word_tokenize \n",
      ">>> word_tokenize('Hello World.') \n",
      "['Hello', 'World', '.'] \n",
      " \n",
      "# 等同于\n",
      ">>> from nltk.tokenize import TreebankWordTokenizer \n",
      ">>> tokenizer = TreebankWordTokenizer() \n",
      ">>> tokenizer.tokenize('Hello World.') \n",
      "['Hello', 'World', '.'] \n",
      " \n",
      "# 等同于\n",
      ">>> import nltk \n",
      ">>> text = \"Hello. Isn't this fun?\" \n",
      ">>> pattern = r\"\\w+|[^\\w\\s]+\" # r：regular expression；双引号\"\"可以用单引号''代替；\\w表示单词字符，等同于字符集合[a-zA-Z0-9_]；+表示一次或者多次，等同于{1,}，即c+ 和 c{1,} 是一个意思；\"|\"：二选一，正则表达式中的\"或\"； [...]：字符集（字符类），其对应的位置可以是字符集中任意字符，例如，a[bcd]表abe、ace和ade；^表示只匹配字符串的开头；\\s匹配单个空格，等同于[\\f\\n\\r\\t\\v]。\n",
      ">>> print nltk.tokenize.regexp_tokenize(text, pattern) \n",
      "['Hello', '.', 'Isn', \"'\", 't', 'this', 'fun', '?'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tokenizing sentences using regular expressions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.tokenize import RegexpTokenizer \n",
      ">>> tokenizer = RegexpTokenizer(\"[\\w']+\") \n",
      ">>> tokenizer.tokenize(\"Can't is a contraction.\") \n",
      "[\"Can't\", 'is', 'a', 'contraction'] \n",
      " \n",
      "# a simple helper function。\n",
      "the class. \n",
      ">>> from nltk.tokenize import regexp_tokenize \n",
      ">>> regexp_tokenize(\"Can't is a contraction.\", \"[\\w']+\") \n",
      "[\"Can't\", 'is', 'a', 'contraction'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Training a sentence tokenizer\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.tokenize import PunktSentenceTokenizer \n",
      ">>> from nltk.corpus import webtext \n",
      ">>> text = webtext.raw('overheard.txt') \n",
      ">>> sent_tokenizer = PunktSentenceTokenizer(text) \n",
      " \n",
      "# Let's compare the results to the default sentence tokenizer, as follows: \n",
      ">>> sents1 = sent_tokenizer.tokenize(text) \n",
      ">>> sents1[0] #请注意，索引从零开始：第0 个元素写作sent[0]，其实是第1 个词\"word1\"；而句子的第9 个元素是\"word10\"。\n",
      "'White guy: So, do you have any plans for this evening?' \n",
      ">>> from nltk.tokenize import sent_tokenize \n",
      ">>> sents2 = sent_tokenize(text) \n",
      ">>> sents2[0] \n",
      "'White guy: So, do you have any plans for this evening?' \n",
      ">>> sents1[678] \n",
      "'Girl: But you already have a Big Mac...' \n",
      ">>> sents2[678] \n",
      "'Girl: But you already have a Big Mac...\\\\nHobo: Oh, this is all \n",
      "theatrical.' \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filtering stopwords in a tokenized sentence\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.corpus import stopwords \n",
      ">>> english_stops = set(stopwords.words('english')) \n",
      ">>> words = [\"Can't\", 'is', 'a', 'contraction'] \n",
      ">>> [word for word in words if word not in english_stops] \n",
      "[\"Can't\", 'contraction'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Looking up synsets for a word in WordNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# 方法一\n",
      ">>> from nltk.corpus import wordnet \n",
      ">>> syn = wordnet.synsets('cookbook')[0] \n",
      ">>> syn.name() \n",
      "'cookbook.n.01' \n",
      ">>> syn.definition() \n",
      "'a book of recipes and cooking directions' \n",
      " \n",
      "# 方法二\n",
      ">>> from nltk.corpus import wordnet \n",
      ">>> syn = wordnet.synsets('motorcar')[0] \n",
      ">>> syn.name \n",
      "<bound method Synset.name of Synset('car.n.01')> \n",
      " \n",
      ">>> from nltk.corpus import wordnet as wn \n",
      ">>> wn.synsets(\"motorcar\") # 括号内可以是单引号\n",
      "[Synset('car.n.01')] \n",
      " \n",
      "# WordNet层次结构（词汇关系）——Hypernym /ˈhaɪpənɪm//hyponym/ ˈhaɪpənɪm / relation——上级概念与从属概念的关系\n",
      ">>> from nltk.corpus import wordnet as wn \n",
      ">>> motorcar = wn.synset('car.n.01') \n",
      ">>> types_of_motorcar = motorcar.hyponyms() #等号两边的表达式不能换位，否则会出现警示：can't assign to function call. \n",
      ">>> types_of_motorcar \n",
      "[Synset('ambulance.n.01'), Synset('beach_wagon.n.01'), Synset('bus.n.04'), Synset('cab.n.03'), Synset('compact.n.03'), Synset('convertible.n.01') ... Synset('stanley_steamer.n.01'), Synset('stock_car.n.01'), Synset('subcompact.n.01'), Synset('touring_car.n.01'), Synset('used-car.n.01')] \n",
      " \n",
      "# 部分整体关系（components (meronyms) holonyms）\n",
      ">>> from nltk.corpus import wordnet as wn \n",
      ">>> wn.synset('tree.n.01').part_meronyms() \n",
      "[Synset('burl.n.02'), Synset('crown.n.07'), Synset('limb.n.02'), Synset('stump.n.01'), Synset('trunk.n.01')] \n",
      " \n",
      "# 反义词关系\n",
      ">>> wn.lemma('beautiful.a.01.beautiful').antonyms() \n",
      "[Lemma('ugly.a.01.ugly')] \n",
      " \n",
      "# 同义词关系\n",
      " \n",
      " \n",
      "# 查看词汇关系和同义词集上定义的其它方法\n",
      ">>> dir(wn.synset('beautiful.a.01')) \n",
      "['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__' ... 'substance_holonyms', 'substance_meronyms', 'topic_domains', 'tree', 'unicode_repr', 'usage_domains', 'verb_groups', 'wup_similarity']\n",
      " \n",
      " \n",
      "# Part-of-Speech (POS) \n",
      ">>> from nltk.corpus import wordnet \n",
      ">>> syn = wordnet.synsets('motorcar')[0] \n",
      ">>> syn.pos() \n",
      "u'n'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Looking up lemmas and synonyms in WordNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#方法一\n",
      ">>> from nltk.corpus import wordnet as wn \n",
      ">>> wn.synset('car.n.01').lemma_names() \n",
      "[u'car', u'auto', u'automobile', u'machine', u'motorcar'] #结果字符串有一个u 前缀表示它们是Unicode 字符串\n",
      " \n",
      ">>> u'motorcar'.encode('utf-8') \n",
      "'motorcar'\n",
      " \n",
      "#方法二\n",
      ">>> a = wn.synset('car.n.01').lemma_names() \n",
      ">>> print a \n",
      "[u'car', u'auto', u'automobile', u'machine', u'motorcar'] \n",
      " \n",
      ">>> wn.synset('car.n.01').definition () \n",
      "u'a motor vehicle with four wheels; usually propelled by an internal combustion engine' \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Calculating WordNet synset similarity\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#量度一：path_similarity--基于上位词层次结构中相互连接的概念之间的最短路径在0-1之间打分（两者之间没有路径返回-1，与自身比较返回1）\n",
      ">>> from nltk.corpus import wordnet as wn \n",
      ">>> right = wn.synset('right_whale.n.01') \n",
      ">>> minke = wn.synset('minke_whale.n.01') \n",
      ">>> right.path_similarity(minke) \n",
      "0.25 \n",
      " \n",
      "#量度二：wup_similarity -- wup_similarity is short for Wu-Palmer Similarity, which is a scoring method based on how similar the word senses are and where the synsets occur relative to each other in the \n",
      "hypernym tree. \n",
      ">>> from nltk.corpus import wordnet \n",
      ">>> cb = wordnet.synset('cookbook.n.01') \n",
      ">>> ib = wordnet.synset('instruction_book.n.01') \n",
      ">>> cb.wup_similarity(ib) \n",
      "0.9166666666666666 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Discovering word collocations (relative to n-gram)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk import bigrams \n",
      ">>> a = \"Jaganadh is testing this application\" \n",
      ">>> tokens = a.split() \n",
      ">>> bigrams(tokens) \n",
      "[('Jaganadh', 'is'), ('is', 'testing'), ('testing', 'this'), ('this', 'application.')] \n",
      " \n",
      "# 如果已分词，则：\n",
      ">>> bigrams(['more', 'is', 'said', 'than', 'done']) \n",
      "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "词频统计\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.book import text1 \n",
      "*** Introductory Examples for the NLTK Book *** \n",
      "Loading text1, ..., text9 and sent1, ..., sent9 \n",
      "Type the name of the text or sentence to view it. \n",
      "Type: 'texts()' or 'sents()' to list the materials. \n",
      "text1: Moby Dick by Herman Melville 1851 \n",
      "text2: Sense and Sensibility by Jane Austen 1811 \n",
      "text3: The Book of Genesis \n",
      "text4: Inaugural Address Corpus \n",
      "text5: Chat Corpus \n",
      "text6: Monty Python and the Holy Grail \n",
      "text7: Wall Street Journal \n",
      "text8: Personals Corpus \n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908 \n",
      ">>> from nltk import FreqDist \n",
      ">>> fdist1 = FreqDist(text1) \n",
      ">>> print fdist1 \n",
      "<FreqDist with 19317 samples and 260819 outcomes> \n",
      ">>> fdist1 \n",
      "FreqDist({u',': 18713, u'the': 13721, u'.': 6862, u'of': 6536, u'and': 6024, u'a': 4569, u'to': 4542, u';': 4072, u'in': 3916, u'that': 2982, ...}) \n",
      ">>> fdist1.most_common(50) \n",
      "[(u',', 18713), (u'the', 13721), (u'.', 6862), (u'of', 6536), (u'and', 6024), (u'a', 4569), (u'to', 4542), (u';', 4072), (u'in', 3916), (u'that', 2982), (u\"'\", 2684), (u'-', 2552), (u'his', 2459), (u'it', 2209), (u'I', 2124), (u's', 1739), (u'is', 1695), (u'he', 1661), (u'with', 1659), (u'was', 1632), (u'as', 1620), (u'\"', 1478), (u'all', 1462), (u'for', 1414), (u'this', 1280), (u'!', 1269), (u'at', 1231), (u'by', 1137), (u'but', 1113), (u'not', 1103), (u'--', 1070), (u'him', 1058), (u'from', 1052), (u'be', 1030), (u'on', 1005), (u'so', 918), (u'whale', 906), (u'one', 889), (u'you', 841), (u'had', 767), (u'have', 760), (u'there', 715), (u'But', 705), (u'or', 697), (u'were', 680), (u'now', 646), (u'which', 640), (u'?', 637), (u'me', 627), (u'like', 624)] \n",
      " \n",
      "# 绘制累积频率图\n",
      ">>> import matplotlib # 不能直接运行from matplotlib import plot。\n",
      ">>> fdist1.plot(50, cumulative=True) # 可能处理的时间比较长。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Stemming words\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# 单个\n",
      ">>> from nltk.stem import PorterStemmer # \"Poter\"是一种词干提取的算法。\n",
      ">>> stemmer = PorterStemmer() \n",
      ">>> stemmer.stem('cooking') \n",
      "u'cook' \n",
      ">>> stemmer.stem('cookery') \n",
      "u'cookeri' # The resulting stem is not always a valid word. For example, the \n",
      "stem of \"cookery\" is \"cookeri\". This is a feature, not a bug. \n",
      " \n",
      "# 多个\n",
      ">>> import nltk \n",
      ">>> stemmer = nltk.PorterStemmer() \n",
      ">>> verbs = ['appears', 'appear', 'appeared', 'calling', 'called'] \n",
      ">>> stems = [] \n",
      ">>> for verb in verbs: \n",
      " stemmed_verb = stemmer.stem(verb) \n",
      " stems.append(stemmed_verb) # 一定要按两次回车键，然后再输入下面的语句。\n",
      " \n",
      " \n",
      ">>> sorted(set(stems)) \n",
      "[u'appear', u'call'] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Lemmatizing words with WordNet\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.stem import WordNetLemmatizer \n",
      ">>> lemmatizer = WordNetLemmatizer() \n",
      ">>> lemmatizer.lemmatize('cooking') \n",
      "'cooking' \n",
      ">>> lemmatizer.lemmatize('cooking', pos='v') \n",
      "u'cook' \n",
      ">>> lemmatizer.lemmatize('cookbooks') \n",
      "u'cookbook' \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replacing words matching regular expressions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# 第一步：新建一个名为\"replacers.py\"的模块(不是在控制台上操作)，放置于安装Python的目录下的\"Lib\"文件夹，也可置于安装Python的根目录下，但最好放在\"Lib\"下，以表明这是一个库（即Python中的模块）。\n",
      "import re \n",
      " \n",
      "replacement_patterns = [ \n",
      " (r'won\\'t', 'will not'), \n",
      " (r'can\\'t', 'cannot'), \n",
      " (r'i\\'m', 'i am'), \n",
      " (r'ain\\'t', 'is not'), \n",
      " (r'(\\w+)\\'ll', '\\g<1> will'), \n",
      " (r'(\\w+)n\\'t', '\\g<1> not'), \n",
      " (r'(\\w+)\\'ve', '\\g<1> have'), \n",
      " (r'(\\w+)\\'s', '\\g<1> is'), \n",
      " (r'(\\w+)\\'re', '\\g<1> are'), \n",
      " (r'(\\w+)\\'d', '\\g<1> would') \n",
      "] \n",
      " \n",
      "class RegexpReplacer(object): \n",
      " def __init__(self, patterns=replacement_patterns): \n",
      " self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns] \n",
      " \n",
      " def replace(self, text): \n",
      " s = text \n",
      " for (pattern, repl) in self.patterns: \n",
      " s = re.sub(pattern, repl, s) \n",
      " return s \n",
      " \n",
      "# 第二步\n",
      ">>> from replacers import RegexpReplacer \n",
      ">>> replacer = RegexpReplacer() \n",
      ">>> replacer.replace(\"can't is a contraction\") \n",
      "'cannot is a contraction' \n",
      ">>> replacer.replace(\"I should've done that thing I didn't do\") \n",
      "'I should have done that thing I did not do'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Accessing Corpora\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.corpus import gutenberg \n",
      ">>> for filename in gutenberg.fileids(): \n",
      " r = gutenberg.raw(filename) \n",
      " w = gutenberg.words(filename) \n",
      " s = gutenberg.sents(filename) \n",
      " v = set(w) \n",
      " print filename, len(r)/len(w), len(w)/len(s), len(w)/len(v) # 要按两次回车键才能显示结果。\n",
      " \n",
      "austen-emma.txt 4 24 24 #语料库的文件名，平均字长，平均句长，每个词平均出现的次数，下同。\n",
      "austen-persuasion.txt 4 26 16 \n",
      "austen-sense.txt 4 28 20 \n",
      "... \n",
      "... \n",
      "shakespeare-macbeth.txt 4 12 5 \n",
      "whitman-leaves.txt 4 36 10\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      ">>> from nltk.book import * \n",
      "*** Introductory Examples for the NLTK Book *** \n",
      "Loading text1, ..., text9 and sent1, ..., sent9 \n",
      "Type the name of the text or sentence to view it. \n",
      "Type: 'texts()' or 'sents()' to list the materials. \n",
      "text1: Moby Dick by Herman Melville 1851 \n",
      "... \n",
      "... \n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908 \n",
      " \n",
      ">>> text1.concordance(\"monstrous\") \n",
      "Displaying 11 of 11 matches: \n",
      "ong the former , one was of a most monstrous size . ... This came towards us , \n",
      "ON OF THE PSALMS . \" Touching that monstrous bulk of the whale or ork we have r \n",
      "... \n",
      "... \n",
      "ght have been rummaged out of this monstrous cabinet there is no telling . But \n",
      "of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "外部文档操作\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# 外部文档操作\n",
      "# 读取一个txt文件（已经新建一个\"good.txt\"文件在D:/Python27下）\n",
      ">>> f = open('document.txt') \n",
      ">>> raw = f.read() \n",
      " \n",
      "#打印\"good.txt\"内容。# 即使打印Walden这样比较大的文件也比较快\n",
      ">>> f = open ('good.txt') \n",
      ">>> print f.read() \n",
      "happy  \n",
      "Lucy \n",
      "Lilei \n",
      " \n",
      ">>> # -*- coding:utf-8 -*- \n",
      ">>> f = open ('语言学.txt') \n",
      ">>> print f.read() \n",
      "计算语言学\n",
      "自然语言处理\n",
      " \n",
      "# 建立自己的语料库，并对语料库里的文件进行检索\n",
      "# 第一步\n",
      ">>> corpus_root = 'D:/Python27/my own data' \n",
      ">>> from nltk.corpus import PlaintextCorpusReader \n",
      ">>> corpus_root = 'D:/Python27/my own data' \n",
      ">>> wordlists = PlaintextCorpusReader(corpus_root, 'Walden.txt') \n",
      ">>> wordlists.fileids() \n",
      "['Walden.txt'] \n",
      "#注意：\n",
      ">>> from nltk.corpus import PlaintextCorpusReader \n",
      ">>> corpus_root = 'D:/Python27/my own data' \n",
      ">>> wordlists = PlaintextCorpusReader(corpus_root, '.*') # .*具有贪婪的性质，首先匹配到不能匹配为止，根据后面的正则表达式，会进行回溯。\n",
      ">>> wordlists.fileids() \n",
      "['Gone with the Wind.txt', 'Walden.txt'] \n",
      " \n",
      "# 第二步\n",
      ">>> n = nltk.word_tokenize(wordlists.raw(fileids=\"Walden.txt\")) \n",
      ">>> complete_Walden = nltk.Text(n) \n",
      ">>> complete_Walden.concordance(\"love\") \n",
      "Displaying 25 of 40 matches: \n",
      "r even to found a school , but so to love wisdom as to live according to its d \n",
      " , perhaps we are led oftener by the love of novelty and a regard for the opin \n",
      "... \n",
      "... \n",
      "eed have you to employ punishments ? Love virtue , and the people will be virt \n",
      "abardine dressed . '' `` Come ye who love , And ye who hate , Children of the \n",
      " \n",
      "# 获取网络文本\n",
      ">>> from urllib import urlopen \n",
      ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\" \n",
      ">>> html = urlopen(url).read() \n",
      ">>> html[:60] \n",
      "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN' \n",
      "# 接下来如果输入print html可以看到HTML 的全部内容，包括meta 元标签、图像标签、map 标 \n",
      "签、JavaScript、表单和表格。\n",
      " \n",
      "# NLTK本来提供了一个辅助函数nltk.clean_html()将HTML 字符串作为参数，返回原始文本；但现在这个函数已经不被支持了，而是用BeautifulSoup的函数get_text()。 \n",
      ">>> import urllib \n",
      ">>> from bs4 import BeautifulSoup \n",
      ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\" \n",
      ">>> html = urllib.urlopen(url).read() \n",
      ">>> soup = BeautifulSoup(html) \n",
      ">>> print soup.get_text() \n",
      " \n",
      "# 对网络文本分词\n",
      ">>> import urllib \n",
      ">>> from bs4 import BeautifulSoup \n",
      ">>> url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\" \n",
      ">>> html = urllib.urlopen(url).read() \n",
      ">>> soup = BeautifulSoup(html) \n",
      ">>> raw = BeautifulSoup.get_text(soup) \n",
      ">>> from nltk.tokenize import word_tokenize \n",
      ">>> token = nltk.word_tokenize(raw) \n",
      ">>> token # 可以在token前加上\"print\"。\n",
      "[u'BBC', u'NEWS', u'|', u'Health', u'|', u'Blondes', u\"'to\", u'die', u'out', u'in', u'200', u\"years'\", u'NEWS', u'SPORT', u'WEATHER', u'WORLD', u'SERVICE', u'A-Z', u'INDEX', ...] \n",
      " \n",
      "# 重点\n",
      ">>> s = [u'r118', u'BB'] \n",
      ">>> [str(item) for item in s] \n",
      "['r118', 'BB']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Default tagging\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# 版本一\n",
      ">>> import nltk \n",
      ">>> text=nltk.word_tokenize(\"We are going out.Just you and me.\") \n",
      ">>> print nltk.pos_tag(text) \n",
      "[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'JJ'), ('you', 'PRP'), ('and', 'CC'), ('me', 'PRP'), ('.', '.')] \n",
      " \n",
      "# 版本二\n",
      ">>> sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\" \n",
      ">>> tokens = nltk.word_tokenize(sentence) \n",
      ">>> tokens \n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.'] \n",
      ">>> tagged = nltk.pos_tag(tokens) \n",
      ">>> tagged \n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), ('Arthur', 'NNP'), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')] \n",
      ">>> tagged[0:6] \n",
      "[('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN')] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Chunking and chinking with regular expressions\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Chunking & Parsing \n",
      "# Chart Parsing 是描述CFG(Context Free Grammar)语法的一种方法，两者不是平行关系。\n",
      "import nltk \n",
      " \n",
      "grammar = r\"\"\" \n",
      "NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and nouns \n",
      "{<NNP>+} # chunk sequences of proper nouns \n",
      "\"\"\" \n",
      " \n",
      "cp = nltk.RegexpParser(grammar) \n",
      " \n",
      "tagged_tokens = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), (\"her\", \"PP$\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")] # 实际运用中，需先对\"Rapunzel let down her golden hair.\"这句进行tokenization. \n",
      " \n",
      "print cp.parse(tagged_tokens) \n",
      " \n",
      "# CFG Parsing\n",
      ">>> import nltk # 这两行代码也可以写成：from nltk import CFG; groucho_grammar = CFG.fromstring(\"\"\"\n",
      ">>> groucho_grammar = nltk.CFG.fromstring(\"\"\" \n",
      " S -> NP VP \n",
      " PP -> P NP \n",
      " NP -> D N | D N PP | 'I' \n",
      " VP -> V NP | V PP \n",
      " D -> 'an' | 'a' | 'my' | 'the' \n",
      " N -> 'elephant' | 'pajamas' \n",
      " V -> 'shot' \n",
      " P -> 'in' \n",
      " \"\"\") \n",
      ">>> sent = \"I shot an elephant in my pajamas\".split() # 有可能实现就分好词了，即：sent = ['I','shot','an','elephant','in','my','pajamas'] \n",
      ">>> parser = nltk.ChartParser(groucho_grammar) # Chart Parsing 是描述CFG语法的一种方法。\n",
      ">>> all_the_parses = parser.parse(sent) \n",
      ">>> all_the_parses \n",
      "<generator object parses at 0x030E8AD0> \n",
      ">>> for parse in all_the_parses: \n",
      " print(parse) \n",
      " \n",
      " \n",
      "(S \n",
      " (NP I) \n",
      " (VP \n",
      " (V shot) \n",
      " (NP (D an) (N elephant) (PP (P in) (NP (D my) (N pajamas)))))) \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " posted on 2018-02-01 13:01 Arroz 阅读(...) 评论(...)  编辑 收藏\n",
      "\n",
      "var allowComments=false,cb_blogId=374554,cb_entryId=8463882,cb_blogApp=currentBlogApp,cb_blogUserGuid='9fd47e1f-a1e3-40ce-7bfc-08d49c352df3',cb_entryCreatedDate='2018/2/1 13:01:00';loadViewCount(cb_entryId);var cb_postType=1;\n",
      "var commentManager = new blogCommentManager();commentManager.renderComments(0);\n",
      "\n",
      "\n",
      "\n",
      "刷新评论刷新页面返回顶部\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "    fixPostBody();\r\n",
      "    setTimeout(function () { incrementViewCount(cb_entryId); }, 50);\r\n",
      "    deliverAdT2();\r\n",
      "    deliverAdC1();\r\n",
      "    deliverAdC2();    \r\n",
      "    loadNewsAndKb();\r\n",
      "    loadBlogSignature();\r\n",
      "    LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);\r\n",
      "    GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);\r\n",
      "    loadOptUnderPost();\r\n",
      "    GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);   \r\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "公告\n",
      "\n",
      "loadBlogNews();\n",
      "\n",
      "loadBlogDefaultCalendar();\n",
      "\n",
      "loadBlogSideColumn();\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Copyright ©2018 Arroz\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\DELL\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "html=urllib.request.urlopen(url).read()\n",
    "soup=BeautifulSoup(html)\n",
    "print (soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " 'Python',\n",
       " 'Text',\n",
       " 'Processing',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " '2.0',\n",
       " 'Cookbook',\n",
       " '>',\n",
       " '代码笔记',\n",
       " '-',\n",
       " 'Arroz',\n",
       " '-',\n",
       " '博客园',\n",
       " 'var',\n",
       " 'currentBlogApp',\n",
       " '=',\n",
       " \"'ArrozZhu\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'cb_enable_mathjax=false',\n",
       " ';',\n",
       " 'var',\n",
       " 'isLogined=false',\n",
       " ';',\n",
       " 'Arroz',\n",
       " '随笔-',\n",
       " '109',\n",
       " '文章-',\n",
       " '24',\n",
       " '评论-',\n",
       " '0',\n",
       " '博客园',\n",
       " '首页',\n",
       " '新随笔',\n",
       " '新文章',\n",
       " '联系',\n",
       " '管理',\n",
       " '订阅',\n",
       " '<',\n",
       " 'Python',\n",
       " 'Text',\n",
       " 'Processing',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " '2.0',\n",
       " 'Cookbook',\n",
       " '>',\n",
       " '代码笔记',\n",
       " '如下是',\n",
       " '<',\n",
       " 'Python',\n",
       " 'Text',\n",
       " 'Processing',\n",
       " 'with',\n",
       " 'NLTK',\n",
       " '2.0',\n",
       " 'Cookbook',\n",
       " '>',\n",
       " '一书部分章节的代码笔记',\n",
       " '.',\n",
       " 'Tokenizing',\n",
       " 'text',\n",
       " 'into',\n",
       " 'sentences',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'para',\n",
       " '=',\n",
       " '``',\n",
       " 'Hello',\n",
       " 'World',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " 'Thanks',\n",
       " 'for',\n",
       " 'buying',\n",
       " 'this',\n",
       " 'book',\n",
       " '.',\n",
       " \"''\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'sent_tokenize',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sent_tokenize',\n",
       " '(',\n",
       " 'para',\n",
       " ')',\n",
       " '#',\n",
       " '``',\n",
       " 'sent_tokenize',\n",
       " \"''\",\n",
       " '是一个函数，下文很多中间带下划线的标识符都指的是函数。',\n",
       " '[',\n",
       " \"'Hello\",\n",
       " 'World',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '``',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'good',\n",
       " 'to',\n",
       " 'see',\n",
       " 'you',\n",
       " '.',\n",
       " '``',\n",
       " ',',\n",
       " \"'Thanks\",\n",
       " 'for',\n",
       " 'buying',\n",
       " 'this',\n",
       " 'book',\n",
       " '.',\n",
       " \"'\",\n",
       " ']',\n",
       " 'Tokenizing',\n",
       " 'sentences',\n",
       " 'into',\n",
       " 'words',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'word_tokenize',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'word_tokenize',\n",
       " '(',\n",
       " \"'Hello\",\n",
       " 'World',\n",
       " '.',\n",
       " \"'\",\n",
       " ')',\n",
       " '[',\n",
       " \"'Hello\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'World\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " \"'\",\n",
       " ']',\n",
       " '#',\n",
       " '等同于',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'TreebankWordTokenizer',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'tokenizer',\n",
       " '=',\n",
       " 'TreebankWordTokenizer',\n",
       " '(',\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'tokenizer.tokenize',\n",
       " '(',\n",
       " \"'Hello\",\n",
       " 'World',\n",
       " '.',\n",
       " \"'\",\n",
       " ')',\n",
       " '[',\n",
       " \"'Hello\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'World\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " \"'\",\n",
       " ']',\n",
       " '#',\n",
       " '等同于',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'import',\n",
       " 'nltk',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'text',\n",
       " '=',\n",
       " '``',\n",
       " 'Hello',\n",
       " '.',\n",
       " 'Is',\n",
       " \"n't\",\n",
       " 'this',\n",
       " 'fun',\n",
       " '?',\n",
       " \"''\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'pattern',\n",
       " '=',\n",
       " 'r',\n",
       " \"''\",\n",
       " '\\\\w+|',\n",
       " '[',\n",
       " '^\\\\w\\\\s',\n",
       " ']',\n",
       " '+',\n",
       " \"''\",\n",
       " '#',\n",
       " 'r：regular',\n",
       " 'expression；双引号',\n",
       " \"''\",\n",
       " \"''\",\n",
       " '可以用单引号',\n",
       " \"''\",\n",
       " '代替；\\\\w表示单词字符，等同于字符集合',\n",
       " '[',\n",
       " 'a-zA-Z0-9_',\n",
       " ']',\n",
       " '；+表示一次或者多次，等同于',\n",
       " '{',\n",
       " '1',\n",
       " ',',\n",
       " '}',\n",
       " '，即c+',\n",
       " '和',\n",
       " 'c',\n",
       " '{',\n",
       " '1',\n",
       " ',',\n",
       " '}',\n",
       " '是一个意思；',\n",
       " \"''\",\n",
       " '|',\n",
       " \"''\",\n",
       " '：二选一，正则表达式中的',\n",
       " \"''\",\n",
       " '或',\n",
       " \"''\",\n",
       " '；',\n",
       " '[',\n",
       " '...',\n",
       " ']',\n",
       " '：字符集（字符类），其对应的位置可以是字符集中任意字符，例如，a',\n",
       " '[',\n",
       " 'bcd',\n",
       " ']',\n",
       " '表abe、ace和ade；^表示只匹配字符串的开头；\\\\s匹配单个空格，等同于',\n",
       " '[',\n",
       " '\\\\f\\\\n\\\\r\\\\t\\\\v',\n",
       " ']',\n",
       " '。',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'print',\n",
       " 'nltk.tokenize.regexp_tokenize',\n",
       " '(',\n",
       " 'text',\n",
       " ',',\n",
       " 'pattern',\n",
       " ')',\n",
       " '[',\n",
       " \"'Hello\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " \"'Isn\",\n",
       " \"'\",\n",
       " ',',\n",
       " '``',\n",
       " \"'\",\n",
       " \"''\",\n",
       " ',',\n",
       " \"'t\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'this\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'fun\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'\",\n",
       " '?',\n",
       " \"'\",\n",
       " ']',\n",
       " 'Tokenizing',\n",
       " 'sentences',\n",
       " 'using',\n",
       " 'regular',\n",
       " 'expressions',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'RegexpTokenizer',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'tokenizer',\n",
       " '=',\n",
       " 'RegexpTokenizer',\n",
       " '(',\n",
       " '``',\n",
       " '[',\n",
       " '\\\\w',\n",
       " \"'\",\n",
       " ']',\n",
       " '+',\n",
       " \"''\",\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'tokenizer.tokenize',\n",
       " '(',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " 'is',\n",
       " 'a',\n",
       " 'contraction',\n",
       " '.',\n",
       " \"''\",\n",
       " ')',\n",
       " '[',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " \"''\",\n",
       " ',',\n",
       " \"'is\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'a\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'contraction\",\n",
       " \"'\",\n",
       " ']',\n",
       " '#',\n",
       " 'a',\n",
       " 'simple',\n",
       " 'helper',\n",
       " 'function。',\n",
       " 'the',\n",
       " 'class',\n",
       " '.',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'regexp_tokenize',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'regexp_tokenize',\n",
       " '(',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " 'is',\n",
       " 'a',\n",
       " 'contraction',\n",
       " '.',\n",
       " '``',\n",
       " ',',\n",
       " '``',\n",
       " '[',\n",
       " '\\\\w',\n",
       " \"'\",\n",
       " ']',\n",
       " '+',\n",
       " \"''\",\n",
       " ')',\n",
       " '[',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " \"''\",\n",
       " ',',\n",
       " \"'is\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'a\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'contraction\",\n",
       " \"'\",\n",
       " ']',\n",
       " 'Training',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'tokenizer',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'PunktSentenceTokenizer',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'webtext',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'text',\n",
       " '=',\n",
       " 'webtext.raw',\n",
       " '(',\n",
       " \"'overheard.txt\",\n",
       " \"'\",\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sent_tokenizer',\n",
       " '=',\n",
       " 'PunktSentenceTokenizer',\n",
       " '(',\n",
       " 'text',\n",
       " ')',\n",
       " '#',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'compare',\n",
       " 'the',\n",
       " 'results',\n",
       " 'to',\n",
       " 'the',\n",
       " 'default',\n",
       " 'sentence',\n",
       " 'tokenizer',\n",
       " ',',\n",
       " 'as',\n",
       " 'follows',\n",
       " ':',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents1',\n",
       " '=',\n",
       " 'sent_tokenizer.tokenize',\n",
       " '(',\n",
       " 'text',\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents1',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " '#',\n",
       " '请注意，索引从零开始：第0',\n",
       " '个元素写作sent',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " '，其实是第1',\n",
       " '个词',\n",
       " \"''\",\n",
       " 'word1',\n",
       " \"''\",\n",
       " '；而句子的第9',\n",
       " '个元素是',\n",
       " \"''\",\n",
       " 'word10',\n",
       " \"''\",\n",
       " '。',\n",
       " \"'White\",\n",
       " 'guy',\n",
       " ':',\n",
       " 'So',\n",
       " ',',\n",
       " 'do',\n",
       " 'you',\n",
       " 'have',\n",
       " 'any',\n",
       " 'plans',\n",
       " 'for',\n",
       " 'this',\n",
       " 'evening',\n",
       " '?',\n",
       " \"'\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.tokenize',\n",
       " 'import',\n",
       " 'sent_tokenize',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents2',\n",
       " '=',\n",
       " 'sent_tokenize',\n",
       " '(',\n",
       " 'text',\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents2',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " \"'White\",\n",
       " 'guy',\n",
       " ':',\n",
       " 'So',\n",
       " ',',\n",
       " 'do',\n",
       " 'you',\n",
       " 'have',\n",
       " 'any',\n",
       " 'plans',\n",
       " 'for',\n",
       " 'this',\n",
       " 'evening',\n",
       " '?',\n",
       " \"'\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents1',\n",
       " '[',\n",
       " '678',\n",
       " ']',\n",
       " \"'Girl\",\n",
       " ':',\n",
       " 'But',\n",
       " 'you',\n",
       " 'already',\n",
       " 'have',\n",
       " 'a',\n",
       " 'Big',\n",
       " 'Mac',\n",
       " '...',\n",
       " \"'\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'sents2',\n",
       " '[',\n",
       " '678',\n",
       " ']',\n",
       " \"'Girl\",\n",
       " ':',\n",
       " 'But',\n",
       " 'you',\n",
       " 'already',\n",
       " 'have',\n",
       " 'a',\n",
       " 'Big',\n",
       " 'Mac',\n",
       " '...',\n",
       " '\\\\\\\\nHobo',\n",
       " ':',\n",
       " 'Oh',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'all',\n",
       " 'theatrical',\n",
       " '.',\n",
       " \"'\",\n",
       " 'Filtering',\n",
       " 'stopwords',\n",
       " 'in',\n",
       " 'a',\n",
       " 'tokenized',\n",
       " 'sentence',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'stopwords',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'english_stops',\n",
       " '=',\n",
       " 'set',\n",
       " '(',\n",
       " 'stopwords.words',\n",
       " '(',\n",
       " \"'english\",\n",
       " \"'\",\n",
       " ')',\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'words',\n",
       " '=',\n",
       " '[',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " \"''\",\n",
       " ',',\n",
       " \"'is\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'a\",\n",
       " \"'\",\n",
       " ',',\n",
       " \"'contraction\",\n",
       " \"'\",\n",
       " ']',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " '[',\n",
       " 'word',\n",
       " 'for',\n",
       " 'word',\n",
       " 'in',\n",
       " 'words',\n",
       " 'if',\n",
       " 'word',\n",
       " 'not',\n",
       " 'in',\n",
       " 'english_stops',\n",
       " ']',\n",
       " '[',\n",
       " '``',\n",
       " 'Ca',\n",
       " \"n't\",\n",
       " \"''\",\n",
       " ',',\n",
       " \"'contraction\",\n",
       " \"'\",\n",
       " ']',\n",
       " 'Looking',\n",
       " 'up',\n",
       " 'synsets',\n",
       " 'for',\n",
       " 'a',\n",
       " 'word',\n",
       " 'in',\n",
       " 'WordNet',\n",
       " '#',\n",
       " '方法一',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'wordnet',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'syn',\n",
       " '=',\n",
       " 'wordnet.synsets',\n",
       " '(',\n",
       " \"'cookbook\",\n",
       " \"'\",\n",
       " ')',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'syn.name',\n",
       " '(',\n",
       " ')',\n",
       " \"'cookbook.n.01\",\n",
       " \"'\",\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'syn.definition',\n",
       " '(',\n",
       " ')',\n",
       " \"'a\",\n",
       " 'book',\n",
       " 'of',\n",
       " 'recipes',\n",
       " 'and',\n",
       " 'cooking',\n",
       " 'directions',\n",
       " \"'\",\n",
       " '#',\n",
       " '方法二',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'wordnet',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'syn',\n",
       " '=',\n",
       " 'wordnet.synsets',\n",
       " '(',\n",
       " \"'motorcar\",\n",
       " \"'\",\n",
       " ')',\n",
       " '[',\n",
       " '0',\n",
       " ']',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'syn.name',\n",
       " '<',\n",
       " 'bound',\n",
       " 'method',\n",
       " 'Synset.name',\n",
       " 'of',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'wordnet',\n",
       " 'as',\n",
       " 'wn',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'wn.synsets',\n",
       " '(',\n",
       " '``',\n",
       " 'motorcar',\n",
       " \"''\",\n",
       " ')',\n",
       " '#',\n",
       " '括号内可以是单引号',\n",
       " '[',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ']',\n",
       " '#',\n",
       " 'WordNet层次结构（词汇关系）——Hypernym',\n",
       " '/ˈhaɪpənɪm//hyponym/',\n",
       " 'ˈhaɪpənɪm',\n",
       " '/',\n",
       " 'relation——上级概念与从属概念的关系',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'wordnet',\n",
       " 'as',\n",
       " 'wn',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'motorcar',\n",
       " '=',\n",
       " 'wn.synset',\n",
       " '(',\n",
       " \"'car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'types_of_motorcar',\n",
       " '=',\n",
       " 'motorcar.hyponyms',\n",
       " '(',\n",
       " ')',\n",
       " '#',\n",
       " '等号两边的表达式不能换位，否则会出现警示：ca',\n",
       " \"n't\",\n",
       " 'assign',\n",
       " 'to',\n",
       " 'function',\n",
       " 'call',\n",
       " '.',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'types_of_motorcar',\n",
       " '[',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'ambulance.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'beach_wagon.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'bus.n.04\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'cab.n.03\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'compact.n.03\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'convertible.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " '...',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'stanley_steamer.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'stock_car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'subcompact.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'touring_car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'used-car.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ']',\n",
       " '#',\n",
       " '部分整体关系（components',\n",
       " '(',\n",
       " 'meronyms',\n",
       " ')',\n",
       " 'holonyms）',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'from',\n",
       " 'nltk.corpus',\n",
       " 'import',\n",
       " 'wordnet',\n",
       " 'as',\n",
       " 'wn',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'wn.synset',\n",
       " '(',\n",
       " \"'tree.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " '.part_meronyms',\n",
       " '(',\n",
       " ')',\n",
       " '[',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'burl.n.02\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'crown.n.07\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'limb.n.02\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'stump.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ',',\n",
       " 'Synset',\n",
       " '(',\n",
       " \"'trunk.n.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ']',\n",
       " '#',\n",
       " '反义词关系',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'wn.lemma',\n",
       " '(',\n",
       " \"'beautiful.a.01.beautiful\",\n",
       " \"'\",\n",
       " ')',\n",
       " '.antonyms',\n",
       " '(',\n",
       " ')',\n",
       " '[',\n",
       " 'Lemma',\n",
       " '(',\n",
       " \"'ugly.a.01.ugly\",\n",
       " \"'\",\n",
       " ')',\n",
       " ']',\n",
       " '#',\n",
       " '同义词关系',\n",
       " '#',\n",
       " '查看词汇关系和同义词集上定义的其它方法',\n",
       " '>',\n",
       " '>',\n",
       " '>',\n",
       " 'dir',\n",
       " '(',\n",
       " 'wn.synset',\n",
       " '(',\n",
       " \"'beautiful.a.01\",\n",
       " \"'\",\n",
       " ')',\n",
       " ')',\n",
       " '[',\n",
       " \"'__class__\",\n",
       " ...]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw=BeautifulSoup.get_text(soup)\n",
    "from nltk.tokenize import word_tokenize\n",
    "token=nltk.word_tokenize(raw)\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  default tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'going', 'out.Just', 'you', 'and', 'me', '.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text=nltk.word_tokenize(\"We are going out.Just you and me.\")\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('out.Just', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('and', 'CC'),\n",
       " ('me', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('out.Just', 'IN')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)[0:4]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
